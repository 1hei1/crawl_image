#!/usr/bin/env python3
"""
åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨å›¾ç‰‡çˆ¬è™«ç³»ç»Ÿçš„åŸºæœ¬åŠŸèƒ½
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from crawler.main_crawler import ImageCrawler


async def basic_crawl_example():
    """åŸºæœ¬çˆ¬å–ç¤ºä¾‹"""
    print("ğŸš€ åŸºæœ¬çˆ¬å–ç¤ºä¾‹")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–çˆ¬è™«
        print("ğŸ“ åˆå§‹åŒ–çˆ¬è™«...")
        crawler = ImageCrawler()
        
        # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
        async def progress_callback(stats):
            print(f"ğŸ“Š è¿›åº¦æ›´æ–°: "
                  f"é¡µé¢ {stats.get('pages_crawled', 0)}, "
                  f"å›¾ç‰‡ {stats.get('images_found', 0)}, "
                  f"ä¸‹è½½ {stats.get('images_downloaded', 0)}")
        
        # çˆ¬å–ç½‘ç«™
        print("ğŸŒ å¼€å§‹çˆ¬å–ç½‘ç«™...")
        result = await crawler.crawl_website(
            url='https://httpbin.org',  # ä½¿ç”¨æµ‹è¯•ç½‘ç«™
            session_name='basic_example',
            progress_callback=progress_callback
        )
        
        # æ˜¾ç¤ºç»“æœ
        if result.get('success', False):
            print("\nâœ… çˆ¬å–æˆåŠŸ!")
            print(f"ğŸ“Š {result.get('summary', '')}")
            
            stats = result.get('stats', {})
            print(f"\nè¯¦ç»†ç»Ÿè®¡:")
            print(f"  â€¢ çˆ¬å–é¡µé¢: {stats.get('pages_crawled', 0)}")
            print(f"  â€¢ å‘ç°å›¾ç‰‡: {stats.get('images_found', 0)}")
            print(f"  â€¢ ä¸‹è½½æˆåŠŸ: {stats.get('images_downloaded', 0)}")
            print(f"  â€¢ ä¸‹è½½å¤±è´¥: {stats.get('images_failed', 0)}")
            print(f"  â€¢ è€—æ—¶: {stats.get('duration', 0):.2f}ç§’")
        else:
            print(f"\nâŒ çˆ¬å–å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")


async def batch_crawl_example():
    """æ‰¹é‡çˆ¬å–ç¤ºä¾‹"""
    print("\nğŸš€ æ‰¹é‡çˆ¬å–ç¤ºä¾‹")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–çˆ¬è™«
        crawler = ImageCrawler()
        
        # å®šä¹‰è¦çˆ¬å–çš„ç½‘ç«™åˆ—è¡¨
        urls = [
            'https://httpbin.org',
            'https://jsonplaceholder.typicode.com',
        ]
        
        print(f"ğŸŒ å¼€å§‹æ‰¹é‡çˆ¬å– {len(urls)} ä¸ªç½‘ç«™...")
        
        # æ‰§è¡Œæ‰¹é‡çˆ¬å–
        results = await crawler.crawl_multiple_websites(urls, 'batch_example')
        
        # ç»Ÿè®¡ç»“æœ
        successful = sum(1 for r in results if r.get('success', False))
        failed = len(results) - successful
        
        print(f"\nğŸ“Š æ‰¹é‡çˆ¬å–å®Œæˆ:")
        print(f"  â€¢ æˆåŠŸ: {successful}")
        print(f"  â€¢ å¤±è´¥: {failed}")
        print(f"  â€¢ æ€»è®¡: {len(results)}")
        
        # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
        print(f"\nè¯¦ç»†ç»“æœ:")
        for i, result in enumerate(results):
            status = "âœ…" if result.get('success', False) else "âŒ"
            url = urls[i] if i < len(urls) else "æœªçŸ¥"
            summary = result.get('summary', result.get('error', 'æ— ä¿¡æ¯'))
            print(f"  {status} {url}")
            print(f"     {summary}")
        
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")


def statistics_example():
    """ç»Ÿè®¡ä¿¡æ¯ç¤ºä¾‹"""
    print("\nğŸš€ ç»Ÿè®¡ä¿¡æ¯ç¤ºä¾‹")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–çˆ¬è™«
        crawler = ImageCrawler()
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = crawler.get_statistics()
        
        print("ğŸ“Š ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯:")
        print(f"  â€¢ æ€»å›¾ç‰‡æ•°: {stats['total_images']}")
        print(f"  â€¢ å·²ä¸‹è½½å›¾ç‰‡: {stats['downloaded_images']}")
        print(f"  â€¢ çˆ¬å–ä¼šè¯æ•°: {stats['total_sessions']}")
        print(f"  â€¢ æ´»è·ƒä»»åŠ¡æ•°: {stats['active_tasks']}")
        
        # æ•°æ®åº“ä¿¡æ¯
        db_info = stats.get('database_info', {})
        print(f"\nğŸ’¾ æ•°æ®åº“ä¿¡æ¯:")
        if isinstance(db_info.get('tables'), dict):
            tables = db_info['tables']
            print(f"  â€¢ å›¾ç‰‡è®°å½•: {tables.get('images', 0)}")
            print(f"  â€¢ åˆ†ç±»è®°å½•: {tables.get('categories', 0)}")
            print(f"  â€¢ æ ‡ç­¾è®°å½•: {tables.get('tags', 0)}")
            print(f"  â€¢ ä¼šè¯è®°å½•: {tables.get('crawl_sessions', 0)}")
        
    except Exception as e:
        print(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")


async def custom_config_example():
    """è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹"""
    print("\nğŸš€ è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹")
    print("=" * 50)
    
    try:
        # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        config_file = Path(__file__).parent.parent / 'config' / 'config.yaml'
        
        if config_file.exists():
            print(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_file}")
            crawler = ImageCrawler(str(config_file))
        else:
            print("ğŸ“ ä½¿ç”¨é»˜è®¤é…ç½®")
            crawler = ImageCrawler()
        
        # æ˜¾ç¤ºé…ç½®æ‘˜è¦
        config_summary = crawler.config_manager.get_config_summary()
        print(f"\nâš™ï¸ é…ç½®æ‘˜è¦:")
        print(f"  â€¢ æ•°æ®åº“ç±»å‹: {config_summary['database_type']}")
        print(f"  â€¢ æœ€å¤§æ·±åº¦: {config_summary['max_depth']}")
        print(f"  â€¢ æœ€å¤§å›¾ç‰‡æ•°: {config_summary['max_images']}")
        print(f"  â€¢ æœ€å¤§å¹¶å‘æ•°: {config_summary['max_concurrent']}")
        print(f"  â€¢ ä¸‹è½½è·¯å¾„: {config_summary['download_path']}")
        print(f"  â€¢ æ—¥å¿—çº§åˆ«: {config_summary['log_level']}")
        print(f"  â€¢ ç¯å¢ƒ: {config_summary['environment']}")
        
    except Exception as e:
        print(f"âŒ é…ç½®ç¤ºä¾‹å¤±è´¥: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å›¾ç‰‡çˆ¬è™«ç³»ç»Ÿä½¿ç”¨ç¤ºä¾‹")
    print("=" * 60)
    
    # è¿è¡Œå„ç§ç¤ºä¾‹
    await basic_crawl_example()
    await batch_crawl_example()
    statistics_example()
    await custom_config_example()
    
    print("\nğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆ!")
    print("\nğŸ’¡ æç¤º:")
    print("  â€¢ æŸ¥çœ‹ docs/USAGE.md è·å–è¯¦ç»†ä½¿ç”¨è¯´æ˜")
    print("  â€¢ è¿è¡Œ python main.py --help æŸ¥çœ‹å‘½ä»¤è¡Œé€‰é¡¹")
    print("  â€¢ è¿è¡Œ python run.py ä½¿ç”¨äº¤äº’å¼ç•Œé¢")


if __name__ == '__main__':
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâš ï¸  ç¤ºä¾‹è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è¿è¡Œç¤ºä¾‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
