#!/usr/bin/env python3
"""
æµ‹è¯•æ‡’åŠ è½½å›¾ç‰‡æå–åŠŸèƒ½
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

from bs4 import BeautifulSoup
from crawler.utils.url_parser import URLParser
from crawler.core.async_crawler import AsyncCrawler

def test_lazy_loading_extraction():
    """æµ‹è¯•æ‡’åŠ è½½å›¾ç‰‡æå–"""
    
    # æ¨¡æ‹ŸåŒ…å«æ‡’åŠ è½½å›¾ç‰‡çš„HTML
    html_content = '''
    <html>
    <body>
        <!-- æ ‡å‡†å›¾ç‰‡ -->
        <img src="https://example.com/normal.jpg" alt="Normal Image" />
        
        <!-- æ‡’åŠ è½½å›¾ç‰‡ - data-original -->
        <img src="/static/images/gray.gif" 
             data-original="https://c.53326.com/d/file/newpc202302/z11cch41yhz.jpg" 
             alt="ç§ƒå¤´é¹°6Ké«˜ç«¯ç”µè„‘æ¡Œé¢å£çº¸" class="lazy" />
        
        <!-- æ‡’åŠ è½½å›¾ç‰‡ - data-src -->
        <img src="placeholder.jpg" 
             data-src="https://example.com/lazy-image.png" 
             class="lazyload" />
        
        <!-- æ‡’åŠ è½½å›¾ç‰‡ - data-lazy-src -->
        <img data-lazy-src="https://example.com/another-lazy.jpg" />
        
        <!-- å“åº”å¼å›¾ç‰‡ - srcset -->
        <img srcset="https://example.com/small.jpg 480w, https://example.com/large.jpg 800w" 
             src="https://example.com/fallback.jpg" />
        
        <!-- èƒŒæ™¯å›¾ç‰‡ -->
        <div style="background-image: url('https://example.com/background.jpg');">Content</div>
        
        <!-- å…¶ä»–å…ƒç´ çš„æ‡’åŠ è½½ -->
        <div data-original="https://example.com/div-image.jpg"></div>
        <a data-src="https://example.com/link-image.png">Link</a>
        
        <!-- éå›¾ç‰‡é“¾æ¥ï¼ˆåº”è¯¥è¢«è¿‡æ»¤ï¼‰ -->
        <img src="https://example.com/script.js" />
        <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" />
    </body>
    </html>
    '''
    
    print("ğŸ§ª æµ‹è¯•æ‡’åŠ è½½å›¾ç‰‡æå–åŠŸèƒ½")
    print("=" * 50)
    
    # åˆ›å»ºè§£æå™¨
    base_url = "https://example.com/"
    url_parser = URLParser(base_url)
    
    # è§£æHTML
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹æ¥ä½¿ç”¨å…¶æ–¹æ³•
    crawler = AsyncCrawler({})
    
    # æå–å›¾ç‰‡
    images = set()
    
    # å¤„ç†imgæ ‡ç­¾
    for img in soup.find_all('img'):
        image_urls = crawler._extract_image_urls_from_element(img, url_parser)
        images.update(image_urls)
        
        # æ˜¾ç¤ºæå–ç»“æœ
        src = img.get('src', '')
        data_original = img.get('data-original', '')
        data_src = img.get('data-src', '')
        data_lazy_src = img.get('data-lazy-src', '')
        srcset = img.get('srcset', '')
        
        print(f"ğŸ“· IMGæ ‡ç­¾:")
        if src:
            print(f"   src: {src}")
        if data_original:
            print(f"   data-original: {data_original}")
        if data_src:
            print(f"   data-src: {data_src}")
        if data_lazy_src:
            print(f"   data-lazy-src: {data_lazy_src}")
        if srcset:
            print(f"   srcset: {srcset}")
        
        if image_urls:
            print(f"   âœ… æå–åˆ°: {list(image_urls)}")
        else:
            print(f"   âŒ æœªæå–åˆ°å›¾ç‰‡")
        print()
    
    # å¤„ç†å…¶ä»–å…ƒç´ 
    for element in soup.find_all(['div', 'span', 'a'], attrs={'data-original': True}):
        image_urls = crawler._extract_image_urls_from_element(element, url_parser)
        images.update(image_urls)
        
        print(f"ğŸ“¦ {element.name.upper()}æ ‡ç­¾:")
        print(f"   data-original: {element.get('data-original')}")
        if image_urls:
            print(f"   âœ… æå–åˆ°: {list(image_urls)}")
        else:
            print(f"   âŒ æœªæå–åˆ°å›¾ç‰‡")
        print()
    
    # å¤„ç†CSSèƒŒæ™¯å›¾ç‰‡
    for element in soup.find_all(attrs={'style': True}):
        style = element.get('style', '')
        if 'background-image' in style:
            bg_urls = crawler._extract_background_images(style, url_parser)
            images.update(bg_urls)
            
            print(f"ğŸ¨ CSSèƒŒæ™¯å›¾ç‰‡:")
            print(f"   style: {style}")
            if bg_urls:
                print(f"   âœ… æå–åˆ°: {list(bg_urls)}")
            else:
                print(f"   âŒ æœªæå–åˆ°å›¾ç‰‡")
            print()
    
    print("=" * 50)
    print(f"ğŸ“Š æ€»ç»“:")
    print(f"   æ€»å…±æå–åˆ° {len(images)} ä¸ªå›¾ç‰‡URL:")
    for i, url in enumerate(sorted(images), 1):
        print(f"   {i}. {url}")
    
    # éªŒè¯ç‰¹å®šçš„æ‡’åŠ è½½å›¾ç‰‡æ˜¯å¦è¢«æå–
    expected_urls = [
        "https://c.53326.com/d/file/newpc202302/z11cch41yhz.jpg",
        "https://example.com/lazy-image.png",
        "https://example.com/another-lazy.jpg",
        "https://example.com/background.jpg"
    ]
    
    print(f"\nğŸ” éªŒè¯å…³é”®æ‡’åŠ è½½å›¾ç‰‡:")
    for url in expected_urls:
        if url in images:
            print(f"   âœ… {url}")
        else:
            print(f"   âŒ {url}")

if __name__ == "__main__":
    test_lazy_loading_extraction()
