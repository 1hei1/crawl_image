"""
å›¾ç‰‡çˆ¬è™«ä¸»ç¨‹åº

æä¾›å‘½ä»¤è¡Œæ¥å£å’Œç¨‹åºå…¥å£
"""

import asyncio
import click
import sys
import os
from pathlib import Path
from typing import Optional
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from crawler.main_crawler import ImageCrawler
from crawler.utils.logger import LoggerManager, ErrorHandler, PerformanceMonitor
from config.manager import ConfigManager


class CrawlerCLI:
    """çˆ¬è™«å‘½ä»¤è¡Œæ¥å£"""
    
    def __init__(self):
        self.crawler: Optional[ImageCrawler] = None
        self.logger_manager: Optional[LoggerManager] = None
        self.error_handler: Optional[ErrorHandler] = None
        self.performance_monitor: Optional[PerformanceMonitor] = None
    
    def initialize(self, config_file: Optional[str] = None, verbose: bool = False):
        """åˆå§‹åŒ–çˆ¬è™«ç³»ç»Ÿ"""
        try:
            # åˆå§‹åŒ–é…ç½®
            config_manager = ConfigManager(config_file)
            settings = config_manager.get_settings()
            
            # å¦‚æœæ˜¯è¯¦ç»†æ¨¡å¼ï¼Œè°ƒæ•´æ—¥å¿—çº§åˆ«
            if verbose:
                settings.logging.level = "DEBUG"
                settings.logging.verbose = True
            
            # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
            self.logger_manager = LoggerManager(settings.logging.__dict__)
            self.logger = self.logger_manager.get_logger("CrawlerCLI")
            
            # åˆå§‹åŒ–é”™è¯¯å¤„ç†å™¨å’Œæ€§èƒ½ç›‘æ§
            self.error_handler = ErrorHandler(self.logger_manager)
            self.performance_monitor = PerformanceMonitor(self.logger_manager)
            
            # åˆå§‹åŒ–çˆ¬è™«
            self.crawler = ImageCrawler(config_file)
            
            self.logger.info("çˆ¬è™«ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def crawl_single_website(self, url: str, session_name: Optional[str] = None):
        """çˆ¬å–å•ä¸ªç½‘ç«™"""
        if not self.crawler:
            raise RuntimeError("çˆ¬è™«æœªåˆå§‹åŒ–")
        
        self.logger.info(f"å¼€å§‹çˆ¬å–ç½‘ç«™: {url}")
        
        # è¿›åº¦å›è°ƒå‡½æ•°
        last_update_time = [0]  # ä½¿ç”¨åˆ—è¡¨æ¥é¿å…é—­åŒ…é—®é¢˜

        async def progress_callback(stats):
            current_time = time.time()
            # æ¯2ç§’æ›´æ–°ä¸€æ¬¡è¿›åº¦ï¼Œé¿å…åˆ·å±
            if current_time - last_update_time[0] >= 2:
                print(f"\rğŸ”„ é¡µé¢: {stats.get('pages_crawled', 0)} | "
                      f"å‘ç°: {stats.get('images_found', 0)} | "
                      f"ä¸‹è½½: {stats.get('images_downloaded', 0)} | "
                      f"å¤±è´¥: {stats.get('images_failed', 0)}", end='', flush=True)
                last_update_time[0] = current_time
        
        try:
            # å¼€å§‹æ€§èƒ½ç›‘æ§
            op_id = self.performance_monitor.start_operation("crawl_website")
            
            # æ‰§è¡Œçˆ¬å–
            result = await self.crawler.crawl_website(
                url=url,
                session_name=session_name,
                progress_callback=progress_callback
            )
            
            # ç»“æŸæ€§èƒ½ç›‘æ§
            self.performance_monitor.end_operation(
                op_id,
                pages_crawled=result.get('stats', {}).get('pages_crawled', 0),
                images_downloaded=result.get('stats', {}).get('images_downloaded', 0)
            )
            
            # è¾“å‡ºç»“æœ
            print()  # æ¢è¡Œï¼Œç»“æŸè¿›åº¦æ˜¾ç¤º
            if result.get('success', False):
                self.logger.info(f"çˆ¬å–å®Œæˆ: {result.get('summary', '')}")
                return result
            else:
                self.logger.error(f"çˆ¬å–å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                return result
                
        except Exception as e:
            error_info = self.error_handler.handle_error(
                e, 
                context={'url': url, 'session_name': session_name},
                operation='crawl_website'
            )
            raise e
    
    def display_statistics(self):
        """æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯"""
        if not self.crawler:
            print("çˆ¬è™«æœªåˆå§‹åŒ–")
            return
        
        stats = self.crawler.get_statistics()
        error_stats = self.error_handler.get_error_statistics()
        perf_report = self.performance_monitor.get_performance_report()
        
        print("\n" + "="*50)
        print("çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯")
        print("="*50)
        
        print(f"æ€»å›¾ç‰‡æ•°: {stats['total_images']}")
        print(f"å·²ä¸‹è½½å›¾ç‰‡: {stats['downloaded_images']}")
        print(f"çˆ¬å–ä¼šè¯æ•°: {stats['total_sessions']}")
        print(f"æ´»è·ƒä»»åŠ¡æ•°: {stats['active_tasks']}")
        
        print("\né”™è¯¯ç»Ÿè®¡:")
        print(f"æ€»é”™è¯¯æ•°: {error_stats['total_errors']}")
        print(f"ç½‘ç»œé”™è¯¯: {error_stats['network_errors']}")
        print(f"è§£æé”™è¯¯: {error_stats['parsing_errors']}")
        print(f"æ–‡ä»¶é”™è¯¯: {error_stats['file_errors']}")
        print(f"æ•°æ®åº“é”™è¯¯: {error_stats['database_errors']}")
        
        print("\næ€§èƒ½ä¿¡æ¯:")
        print(f"æ´»è·ƒæ“ä½œæ•°: {perf_report['active_operations']}")
        if perf_report['current_resource_usage']:
            usage = perf_report['current_resource_usage']
            print(f"CPUä½¿ç”¨ç‡: {usage.get('cpu_percent', 0):.1f}%")
            print(f"å†…å­˜ä½¿ç”¨: {usage.get('memory_mb', 0):.1f}MB")
        
        print("="*50)

    async def clean_data(self, clean_images: bool = False, clean_database: bool = False):
        """
        æ¸…ç©ºæ•°æ®

        Args:
            clean_images: æ˜¯å¦æ¸…ç©ºå›¾ç‰‡æ–‡ä»¶
            clean_database: æ˜¯å¦æ¸…ç©ºæ•°æ®åº“è®°å½•

        Returns:
            æ¸…ç†ç»“æœ
        """
        try:
            result = {
                'success': True,
                'images_deleted': 0,
                'database_cleared': False,
                'records_deleted': 0
            }

            # æ¸…ç©ºå›¾ç‰‡æ–‡ä»¶
            if clean_images:
                images_path = Path(self.crawler.settings.crawler.download_path)
                if images_path.exists():
                    import shutil
                    deleted_count = 0

                    # è®¡ç®—è¦åˆ é™¤çš„æ–‡ä»¶æ•°
                    for file_path in images_path.rglob('*'):
                        if file_path.is_file():
                            deleted_count += 1

                    # åˆ é™¤æ•´ä¸ªç›®å½•
                    shutil.rmtree(images_path)

                    # é‡æ–°åˆ›å»ºç©ºç›®å½•
                    images_path.mkdir(parents=True, exist_ok=True)

                    result['images_deleted'] = deleted_count
                    self.logger.info(f"å·²åˆ é™¤ {deleted_count} ä¸ªå›¾ç‰‡æ–‡ä»¶")

            # æ¸…ç©ºæ•°æ®åº“è®°å½•
            if clean_database:
                records_deleted = await self._clear_database_records()
                result['database_cleared'] = True
                result['records_deleted'] = records_deleted
                self.logger.info(f"å·²æ¸…ç©º {records_deleted} æ¡æ•°æ®åº“è®°å½•")

            return result

        except Exception as e:
            self.logger.error(f"æ¸…ç†æ•°æ®å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    async def _clear_database_records(self):
        """æ¸…ç©ºæ•°æ®åº“è®°å½•"""
        try:
            from database.models.image import ImageModel
            from database.models.category import CategoryModel, TagModel
            from database.models.crawl_session import CrawlSessionModel

            # ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨çš„ä¼šè¯ä¸Šä¸‹æ–‡
            with self.crawler.db_manager.get_session() as session:
                records_deleted = 0

                # åˆ é™¤å›¾ç‰‡è®°å½•
                image_count = session.query(ImageModel).count()
                session.query(ImageModel).delete()
                records_deleted += image_count

                # åˆ é™¤çˆ¬å–ä¼šè¯è®°å½•
                session_count = session.query(CrawlSessionModel).count()
                session.query(CrawlSessionModel).delete()
                records_deleted += session_count

                # åˆ é™¤æ ‡ç­¾è®°å½•
                tag_count = session.query(TagModel).count()
                session.query(TagModel).delete()
                records_deleted += tag_count

                # é‡ç½®åˆ†ç±»è®°å½•ï¼ˆä¿ç•™é»˜è®¤åˆ†ç±»ï¼Œä½†æ¸…ç©ºç»Ÿè®¡ï¼‰
                categories = session.query(CategoryModel).all()
                for category in categories:
                    category.image_count = 0
                    category.total_size = 0

                session.commit()
                return records_deleted

        except Exception as e:
            self.logger.error(f"æ¸…ç©ºæ•°æ®åº“è®°å½•å¤±è´¥: {e}")
            raise e


# CLIå‘½ä»¤å®šä¹‰
@click.group()
@click.option('--config', '-c', help='é…ç½®æ–‡ä»¶è·¯å¾„')
@click.option('--verbose', '-v', is_flag=True, help='è¯¦ç»†è¾“å‡ºæ¨¡å¼')
@click.option('--quiet', '-q', is_flag=True, help='é™é»˜æ¨¡å¼ï¼ˆæœ€å°åŒ–è¾“å‡ºï¼‰')
@click.pass_context
def cli(ctx, config, verbose, quiet):
    """å›¾ç‰‡çˆ¬è™«å‘½ä»¤è¡Œå·¥å…·"""
    ctx.ensure_object(dict)

    # å¦‚æœæ˜¯é™é»˜æ¨¡å¼ï¼Œä½¿ç”¨é™é»˜é…ç½®
    if quiet and not config:
        config = str(Path(__file__).parent / 'config' / 'quiet_config.yaml')

    # åˆå§‹åŒ–CLI
    crawler_cli = CrawlerCLI()
    if not crawler_cli.initialize(config, verbose):
        ctx.exit(1)

    ctx.obj['cli'] = crawler_cli


@cli.command()
@click.argument('url')
@click.option('--name', '-n', help='ä¼šè¯åç§°')
@click.pass_context
def crawl(ctx, url, name):
    """çˆ¬å–æŒ‡å®šç½‘ç«™çš„å›¾ç‰‡"""
    crawler_cli = ctx.obj['cli']
    
    try:
        # è¿è¡Œå¼‚æ­¥çˆ¬å–
        result = asyncio.run(crawler_cli.crawl_single_website(url, name))
        
        if result.get('success', False):
            click.echo(f"âœ… çˆ¬å–æˆåŠŸ: {result.get('summary', '')}")
            click.echo(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            stats = result.get('stats', {})
            click.echo(f"   - é¡µé¢æ•°: {stats.get('pages_crawled', 0)}")
            click.echo(f"   - å‘ç°å›¾ç‰‡: {stats.get('images_found', 0)}")
            click.echo(f"   - ä¸‹è½½æˆåŠŸ: {stats.get('images_downloaded', 0)}")
            click.echo(f"   - ä¸‹è½½å¤±è´¥: {stats.get('images_failed', 0)}")
            click.echo(f"   - è€—æ—¶: {stats.get('duration', 0):.2f}ç§’")
        else:
            click.echo(f"âŒ çˆ¬å–å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            ctx.exit(1)
            
    except KeyboardInterrupt:
        click.echo("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        crawler_cli.crawler.stop_all_tasks()
        ctx.exit(0)
    except Exception as e:
        click.echo(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        ctx.exit(1)


@cli.command()
@click.argument('urls', nargs=-1, required=True)
@click.option('--name', '-n', help='æ‰¹æ¬¡åç§°å‰ç¼€')
@click.pass_context
def batch(ctx, urls, name):
    """æ‰¹é‡çˆ¬å–å¤šä¸ªç½‘ç«™"""
    crawler_cli = ctx.obj['cli']
    
    try:
        click.echo(f"å¼€å§‹æ‰¹é‡çˆ¬å– {len(urls)} ä¸ªç½‘ç«™...")
        
        # è¿è¡Œæ‰¹é‡çˆ¬å–
        results = asyncio.run(crawler_cli.crawler.crawl_multiple_websites(list(urls), name))
        
        # ç»Ÿè®¡ç»“æœ
        successful = sum(1 for r in results if r.get('success', False))
        failed = len(results) - successful
        
        click.echo(f"\nğŸ“Š æ‰¹é‡çˆ¬å–å®Œæˆ:")
        click.echo(f"   - æˆåŠŸ: {successful}")
        click.echo(f"   - å¤±è´¥: {failed}")
        click.echo(f"   - æ€»è®¡: {len(results)}")
        
        # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
        for i, result in enumerate(results):
            status = "âœ…" if result.get('success', False) else "âŒ"
            url = urls[i] if i < len(urls) else "æœªçŸ¥"
            summary = result.get('summary', result.get('error', 'æ— ä¿¡æ¯'))
            click.echo(f"   {status} {url}: {summary}")
            
    except KeyboardInterrupt:
        click.echo("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        crawler_cli.crawler.stop_all_tasks()
        ctx.exit(0)
    except Exception as e:
        click.echo(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        ctx.exit(1)


@cli.command()
@click.pass_context
def stats(ctx):
    """æ˜¾ç¤ºçˆ¬è™«ç»Ÿè®¡ä¿¡æ¯"""
    crawler_cli = ctx.obj['cli']
    crawler_cli.display_statistics()


@cli.command()
@click.pass_context
def stop(ctx):
    """åœæ­¢æ‰€æœ‰æ´»è·ƒçš„çˆ¬å–ä»»åŠ¡"""
    crawler_cli = ctx.obj['cli']
    crawler_cli.crawler.stop_all_tasks()
    click.echo("âœ… å·²å‘é€åœæ­¢ä¿¡å·ç»™æ‰€æœ‰æ´»è·ƒä»»åŠ¡")


@cli.command()
@click.option('--output', '-o', default='config/config.yaml', help='è¾“å‡ºé…ç½®æ–‡ä»¶è·¯å¾„')
@click.pass_context
def init_config(ctx, output):
    """ç”Ÿæˆé»˜è®¤é…ç½®æ–‡ä»¶"""
    try:
        from shutil import copy2
        
        # å¤åˆ¶æ¨¡æ¿é…ç½®æ–‡ä»¶
        template_path = Path(__file__).parent / 'config' / 'templates' / 'config.yaml'
        output_path = Path(output)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        copy2(template_path, output_path)
        click.echo(f"âœ… é…ç½®æ–‡ä»¶å·²ç”Ÿæˆ: {output_path}")
        click.echo("è¯·æ ¹æ®éœ€è¦ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°")
        
    except Exception as e:
        click.echo(f"âŒ ç”Ÿæˆé…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        ctx.exit(1)


@cli.command()
@click.option('--images', is_flag=True, help='æ¸…ç©ºä¸‹è½½çš„å›¾ç‰‡æ–‡ä»¶')
@click.option('--database', is_flag=True, help='æ¸…ç©ºæ•°æ®åº“è®°å½•')
@click.option('--all', is_flag=True, help='æ¸…ç©ºæ‰€æœ‰æ•°æ®ï¼ˆå›¾ç‰‡+æ•°æ®åº“ï¼‰')
@click.option('--force', is_flag=True, help='å¼ºåˆ¶æ¸…ç©ºï¼Œä¸è¯¢é—®ç¡®è®¤')
@click.pass_context
def clean(ctx, images, database, all, force):
    """æ¸…ç©ºä¸‹è½½çš„å›¾ç‰‡å’Œæ•°æ®åº“è®°å½•"""
    crawler_cli = ctx.obj['cli']

    # å¦‚æœæŒ‡å®šäº†--allï¼Œåˆ™æ¸…ç©ºæ‰€æœ‰
    if all:
        images = True
        database = True

    # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»ä½•é€‰é¡¹ï¼Œé»˜è®¤è¯¢é—®
    if not images and not database:
        click.echo("è¯·æŒ‡å®šè¦æ¸…ç©ºçš„å†…å®¹:")
        click.echo("  --images    æ¸…ç©ºä¸‹è½½çš„å›¾ç‰‡æ–‡ä»¶")
        click.echo("  --database  æ¸…ç©ºæ•°æ®åº“è®°å½•")
        click.echo("  --all       æ¸…ç©ºæ‰€æœ‰æ•°æ®")
        return

    # ç¡®è®¤æ“ä½œ
    if not force:
        operations = []
        if images:
            operations.append("ä¸‹è½½çš„å›¾ç‰‡æ–‡ä»¶")
        if database:
            operations.append("æ•°æ®åº“è®°å½•")

        click.echo(f"âš ï¸  å³å°†æ¸…ç©º: {', '.join(operations)}")
        if not click.confirm("ç¡®å®šè¦ç»§ç»­å—ï¼Ÿæ­¤æ“ä½œä¸å¯æ¢å¤"):
            click.echo("âŒ æ“ä½œå·²å–æ¶ˆ")
            return

    try:
        result = asyncio.run(crawler_cli.clean_data(images, database))

        if result.get('success', False):
            click.echo("âœ… æ¸…ç†å®Œæˆ!")
            if result.get('images_deleted', 0) > 0:
                click.echo(f"   åˆ é™¤å›¾ç‰‡æ–‡ä»¶: {result['images_deleted']} ä¸ª")
            if result.get('database_cleared', False):
                click.echo(f"   æ¸…ç©ºæ•°æ®åº“è®°å½•: {result['records_deleted']} æ¡")
        else:
            click.echo(f"âŒ æ¸…ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            ctx.exit(1)

    except Exception as e:
        click.echo(f"âŒ æ¸…ç†å¤±è´¥: {e}")
        ctx.exit(1)


@cli.command()
@click.pass_context
def test(ctx):
    """æµ‹è¯•çˆ¬è™«ç³»ç»Ÿ"""
    crawler_cli = ctx.obj['cli']

    click.echo("ğŸ”§ æµ‹è¯•çˆ¬è™«ç³»ç»Ÿ...")

    try:
        # æµ‹è¯•æ•°æ®åº“è¿æ¥
        if crawler_cli.crawler.db_manager.test_connection():
            click.echo("âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸")
        else:
            click.echo("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥")
            return

        # æµ‹è¯•ç½‘ç»œè¿æ¥
        click.echo("ğŸŒ æµ‹è¯•ç½‘ç»œè¿æ¥...")
        test_result = asyncio.run(test_network_connection())
        if test_result:
            click.echo("âœ… ç½‘ç»œè¿æ¥æ­£å¸¸")
        else:
            click.echo("âŒ ç½‘ç»œè¿æ¥å¤±è´¥")

        click.echo("âœ… ç³»ç»Ÿæµ‹è¯•å®Œæˆ")

    except Exception as e:
        click.echo(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


async def test_network_connection():
    """æµ‹è¯•ç½‘ç»œè¿æ¥"""
    import aiohttp
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get('https://httpbin.org/get', timeout=10) as response:
                return response.status == 200
    except Exception:
        return False


if __name__ == '__main__':
    cli()
