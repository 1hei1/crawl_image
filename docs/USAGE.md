# ä½¿ç”¨è¯´æ˜

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨æ™ºèƒ½å›¾ç‰‡çˆ¬è™«ç³»ç»Ÿã€‚

## ğŸ“‹ ç›®å½•

- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
- [å‘½ä»¤è¡Œä½¿ç”¨](#å‘½ä»¤è¡Œä½¿ç”¨)
- [Python API](#python-api)
- [é«˜çº§åŠŸèƒ½](#é«˜çº§åŠŸèƒ½)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

ç¡®ä¿æ‚¨çš„ç³»ç»Ÿæ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š
- Python 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬
- è‡³å°‘ 2GB å¯ç”¨å†…å­˜
- ç¨³å®šçš„ç½‘ç»œè¿æ¥

### 2. å®‰è£…ä¾èµ–

```bash
cd image_crawler
pip install -r requirements.txt
```

### 3. åˆå§‹åŒ–é…ç½®

```bash
python main.py init-config
```

è¿™å°†åœ¨ `config/` ç›®å½•ä¸‹ç”Ÿæˆé»˜è®¤é…ç½®æ–‡ä»¶ `config.yaml`ã€‚

### 4. ç¬¬ä¸€æ¬¡çˆ¬å–

ä½¿ç”¨äº¤äº’å¼ç•Œé¢ï¼š
```bash
python run.py
```

æˆ–ä½¿ç”¨å‘½ä»¤è¡Œï¼š
```bash
python main.py crawl https://example.com
```

## âš™ï¸ é…ç½®è¯´æ˜

### é…ç½®æ–‡ä»¶ç»“æ„

é…ç½®æ–‡ä»¶é‡‡ç”¨ YAML æ ¼å¼ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š

```yaml
# é¡¹ç›®åŸºæœ¬ä¿¡æ¯
project_name: "å›¾ç‰‡çˆ¬è™«ç³»ç»Ÿ"
version: "1.0.0"
environment: "development"
debug: true

# æ•°æ®åº“é…ç½®
database:
  type: "sqlite"
  sqlite_path: "data/images.db"

# çˆ¬è™«é…ç½®
crawler:
  max_depth: 3
  max_images: 1000
  max_concurrent: 10

# åçˆ¬è™«é…ç½®
anti_crawler:
  use_random_user_agent: true
  random_delay: true

# åˆ†ç±»é…ç½®
classification:
  enable_auto_classification: true

# æ—¥å¿—é…ç½®
logging:
  level: "INFO"
  log_file: "logs/crawler.log"
```

### é‡è¦é…ç½®é¡¹è¯´æ˜

#### æ•°æ®åº“é…ç½®

**SQLiteï¼ˆæ¨èç”¨äºå¼€å‘å’Œå°è§„æ¨¡ä½¿ç”¨ï¼‰**
```yaml
database:
  type: "sqlite"
  sqlite_path: "data/images.db"
```

**PostgreSQLï¼ˆæ¨èç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰**
```yaml
database:
  type: "postgresql"
  host: "localhost"
  port: 5432
  database: "image_crawler"
  username: "your_username"
  password: "your_password"
```

#### çˆ¬è™«æ€§èƒ½é…ç½®

```yaml
crawler:
  max_depth: 3              # çˆ¬å–æ·±åº¦ï¼Œå»ºè®® 1-5
  max_images: 1000          # æœ€å¤§å›¾ç‰‡æ•°é‡
  max_concurrent: 10        # å¹¶å‘æ•°ï¼Œæ ¹æ®ç½‘ç»œå’Œç›®æ ‡æœåŠ¡å™¨è°ƒæ•´
  request_delay: 1.0        # è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰
  timeout: 30               # è¯·æ±‚è¶…æ—¶æ—¶é—´
```

#### åçˆ¬è™«é…ç½®

```yaml
anti_crawler:
  use_random_user_agent: true    # éšæœº User-Agent
  use_proxy: false               # æ˜¯å¦ä½¿ç”¨ä»£ç†
  proxy_list: []                 # ä»£ç†åˆ—è¡¨
  random_delay: true             # éšæœºå»¶è¿Ÿ
  min_delay: 0.5                 # æœ€å°å»¶è¿Ÿ
  max_delay: 3.0                 # æœ€å¤§å»¶è¿Ÿ
```

### ç¯å¢ƒå˜é‡é…ç½®

æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ç¯å¢ƒå˜é‡è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ï¼š

```bash
# æ•°æ®åº“é…ç½®
export DB_TYPE=postgresql
export DB_HOST=localhost
export DB_PORT=5432
export DB_NAME=image_crawler
export DB_USER=username
export DB_PASSWORD=password

# çˆ¬è™«é…ç½®
export CRAWLER_MAX_DEPTH=5
export CRAWLER_MAX_IMAGES=2000
export CRAWLER_MAX_CONCURRENT=20

# æ—¥å¿—é…ç½®
export LOG_LEVEL=DEBUG
export LOG_FILE=logs/debug.log
```

## ğŸ’» å‘½ä»¤è¡Œä½¿ç”¨

### åŸºæœ¬å‘½ä»¤

```bash
# æŸ¥çœ‹å¸®åŠ©
python main.py --help

# çˆ¬å–å•ä¸ªç½‘ç«™
python main.py crawl https://example.com

# æŒ‡å®šä¼šè¯åç§°
python main.py crawl https://example.com --name "example_session"

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®æ–‡ä»¶
python main.py -c my_config.yaml crawl https://example.com

# è¯¦ç»†è¾“å‡ºæ¨¡å¼
python main.py -v crawl https://example.com
```

### æ‰¹é‡çˆ¬å–

```bash
# æ‰¹é‡çˆ¬å–å¤šä¸ªç½‘ç«™
python main.py batch https://site1.com https://site2.com https://site3.com

# æŒ‡å®šæ‰¹æ¬¡åç§°
python main.py batch --name "batch_001" https://site1.com https://site2.com
```

### ç³»ç»Ÿç®¡ç†

```bash
# æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
python main.py stats

# åœæ­¢æ‰€æœ‰æ´»è·ƒä»»åŠ¡
python main.py stop

# æµ‹è¯•ç³»ç»Ÿ
python main.py test

# ç”Ÿæˆé…ç½®æ–‡ä»¶
python main.py init-config --output my_config.yaml
```

## ğŸ Python API

### åŸºæœ¬ä½¿ç”¨

```python
import asyncio
from crawler.main_crawler import ImageCrawler

async def main():
    # åˆå§‹åŒ–çˆ¬è™«
    crawler = ImageCrawler('config/config.yaml')
    
    # çˆ¬å–ç½‘ç«™
    result = await crawler.crawl_website('https://example.com')
    
    if result['success']:
        print(f"çˆ¬å–æˆåŠŸ: {result['summary']}")
        print(f"ä¸‹è½½å›¾ç‰‡: {result['stats']['images_downloaded']}")
    else:
        print(f"çˆ¬å–å¤±è´¥: {result['error']}")

# è¿è¡Œ
asyncio.run(main())
```

### å¸¦è¿›åº¦å›è°ƒçš„çˆ¬å–

```python
async def progress_callback(stats):
    print(f"è¿›åº¦: {stats['pages_crawled']} é¡µé¢, {stats['images_downloaded']} å›¾ç‰‡")

async def main():
    crawler = ImageCrawler()
    
    result = await crawler.crawl_website(
        url='https://example.com',
        session_name='my_session',
        progress_callback=progress_callback
    )
```

### æ‰¹é‡çˆ¬å–

```python
async def main():
    crawler = ImageCrawler()
    
    urls = [
        'https://site1.com',
        'https://site2.com',
        'https://site3.com'
    ]
    
    results = await crawler.crawl_multiple_websites(urls, 'batch_session')
    
    for result in results:
        if result['success']:
            print(f"âœ… {result['start_url']}: {result['summary']}")
        else:
            print(f"âŒ {result['start_url']}: {result['error']}")
```

### è·å–ç»Ÿè®¡ä¿¡æ¯

```python
def main():
    crawler = ImageCrawler()
    stats = crawler.get_statistics()
    
    print(f"æ€»å›¾ç‰‡æ•°: {stats['total_images']}")
    print(f"å·²ä¸‹è½½: {stats['downloaded_images']}")
    print(f"æ€»ä¼šè¯: {stats['total_sessions']}")
```

## ğŸ”§ é«˜çº§åŠŸèƒ½

### è‡ªå®šä¹‰å›¾ç‰‡åˆ†ç±»è§„åˆ™

ç¼–è¾‘é…ç½®æ–‡ä»¶ä¸­çš„åˆ†ç±»è§„åˆ™ï¼š

```yaml
classification:
  enable_auto_classification: true
  
  # åŸºäºæ–‡ä»¶åçš„åˆ†ç±»è§„åˆ™
  filename_rules:
    "äº§å“å›¾ç‰‡":
      - "product"
      - "item"
      - "goods"
    "ç”¨æˆ·å¤´åƒ":
      - "avatar"
      - "profile"
      - "user"
  
  # åŸºäºå°ºå¯¸çš„åˆ†ç±»è§„åˆ™
  size_rules:
    "å¤§å›¾":
      min_width: 1200
      min_height: 800
    "å°å›¾æ ‡":
      max_width: 100
      max_height: 100
```

### ä½¿ç”¨ä»£ç†

```yaml
anti_crawler:
  use_proxy: true
  proxy_list:
    - "http://proxy1:8080"
    - "http://proxy2:8080"
    - "socks5://proxy3:1080"
  proxy_rotation: true
```

### è‡ªå®šä¹‰ User-Agent

```yaml
anti_crawler:
  use_random_user_agent: true
  custom_user_agents:
    - "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    - "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
```

### æ–‡ä»¶è¿‡æ»¤

```yaml
crawler:
  # å…è®¸çš„æ–‡ä»¶æ‰©å±•å
  allowed_extensions:
    - ".jpg"
    - ".png"
    - ".webp"
  
  # æ–‡ä»¶å¤§å°é™åˆ¶
  min_file_size: 1024      # 1KB
  max_file_size: 10485760  # 10MB
  
  # å›¾ç‰‡å°ºå¯¸é™åˆ¶
  min_width: 100
  min_height: 100
  max_width: 5000
  max_height: 5000
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**1. ç½‘ç»œè¿æ¥å¤±è´¥**
```
é”™è¯¯: aiohttp.ClientConnectorError
è§£å†³: æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œå°è¯•ä½¿ç”¨ä»£ç†
```

**2. å›¾ç‰‡ä¸‹è½½å¤±è´¥**
```
é”™è¯¯: HTTP 403 Forbidden
è§£å†³: è°ƒæ•´ User-Agent è®¾ç½®ï¼Œå¢åŠ è¯·æ±‚å»¶è¿Ÿ
```

**3. æ•°æ®åº“è¿æ¥å¤±è´¥**
```
é”™è¯¯: sqlalchemy.exc.OperationalError
è§£å†³: æ£€æŸ¥æ•°æ®åº“é…ç½®å’ŒæœåŠ¡çŠ¶æ€
```

**4. å†…å­˜ä½¿ç”¨è¿‡é«˜**
```
è§£å†³: å‡å°‘å¹¶å‘æ•°ï¼Œè®¾ç½®å›¾ç‰‡å¤§å°é™åˆ¶
```

### è°ƒè¯•æ¨¡å¼

å¯ç”¨è¯¦ç»†æ—¥å¿—ï¼š
```bash
python main.py -v crawl https://example.com
```

æˆ–åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®ï¼š
```yaml
logging:
  level: "DEBUG"
  verbose: true
```

### æ€§èƒ½ä¼˜åŒ–

**æé«˜ä¸‹è½½é€Ÿåº¦ï¼š**
- å¢åŠ å¹¶å‘æ•°ï¼ˆæ³¨æ„æœåŠ¡å™¨è´Ÿè½½ï¼‰
- ä½¿ç”¨æ›´å¿«çš„ç½‘ç»œè¿æ¥
- å¯ç”¨ä»£ç†æ± 

**å‡å°‘èµ„æºä½¿ç”¨ï¼š**
- è®¾ç½®åˆç†çš„å›¾ç‰‡å¤§å°é™åˆ¶
- å¯ç”¨å›¾ç‰‡æ ¼å¼è¿‡æ»¤
- å®šæœŸæ¸…ç†ä¸‹è½½ç›®å½•

### æ—¥å¿—åˆ†æ

æ—¥å¿—æ–‡ä»¶ä½ç½®ï¼š
- æ™®é€šæ—¥å¿—: `logs/crawler.log`
- é”™è¯¯æ—¥å¿—: `logs/crawler_error.log`

æŸ¥çœ‹å®æ—¶æ—¥å¿—ï¼š
```bash
tail -f logs/crawler.log
```

## ğŸ“ è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·ï¼š

1. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯
2. æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦æ­£ç¡®
3. è¿è¡Œç³»ç»Ÿæµ‹è¯•: `python main.py test`
4. æŸ¥çœ‹é¡¹ç›®æ–‡æ¡£å’Œç¤ºä¾‹
5. æäº¤ GitHub Issue

---

æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒé¡¹ç›® README æ–‡ä»¶ã€‚
