
import asyncio
import os
import time
from fastapi import FastAPI, HTTPException, BackgroundTasks, Request
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import uvicorn
from pathlib import Path
from typing import List

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨ Python è·¯å¾„ä¸­
import sys
sys.path.insert(0, str(Path(__file__).parent))

from crawler.main_crawler import ImageCrawler
from database.models.image import ImageModel
from database.distributed_ha_manager import DistributedHAManager
from config.ha_config_loader import load_ha_config
from storage.distributed_file_manager import DistributedFileManager
from storage.file_sync_api import create_file_sync_api
from loguru import logger

# --------------------------------------------------------------------------
# æ—¥å¿—æµå’Œåº”ç”¨è®¾ç½®
# --------------------------------------------------------------------------

# åˆ›å»ºä¸€ä¸ªé˜Ÿåˆ—æ¥å­˜å‚¨æ—¥å¿—æ¶ˆæ¯
log_queue = asyncio.Queue()

# è‡ªå®šä¹‰æ—¥å¿—å¤„ç†å™¨ï¼Œå°†æ—¥å¿—æ”¾å…¥é˜Ÿåˆ—
async def queue_sink(message):
    await log_queue.put(message.strip())

# é…ç½® loguru
logger.add(
    "logs/crawler.log",
    level="INFO",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}",
    enqueue=True,
    rotation="10 MB"
)
logger.add(queue_sink, level="INFO", format="{message}")

# åˆå§‹åŒ– FastAPI åº”ç”¨
app = FastAPI(
    title="å›¾ç‰‡çˆ¬è™« API",
    description="æä¾›ä¸€ä¸ªç”¨äºæ§åˆ¶å’Œç›‘æ§å›¾ç‰‡çˆ¬è™«çš„ Web APIã€‚",
    version="1.0.0"
)

# --------------------------------------------------------------------------
# å…¨å±€å˜é‡å’ŒçŠ¶æ€
# --------------------------------------------------------------------------

# åˆå§‹åŒ–åˆ†å¸ƒå¼HAç®¡ç†å™¨
ha_manager = None
file_manager = None
try:
    # åŠ è½½HAé…ç½®
    nodes, local_node_name, config = load_ha_config()
    ha_manager = DistributedHAManager(nodes, local_node_name)
    ha_manager.start_monitoring()

    # åˆå§‹åŒ–åˆ†å¸ƒå¼æ–‡ä»¶ç®¡ç†å™¨
    servers = [
        {
            "host": node.server.host,
            "port": node.server.api_port,
            "name": node.name
        }
        for node in nodes if node.name != local_node_name
    ]

    # æš‚æ—¶è·³è¿‡æ–‡ä»¶ç®¡ç†å™¨åˆå§‹åŒ–ï¼Œé¿å…å¼‚æ­¥é—®é¢˜
    file_manager = None
    # file_manager = DistributedFileManager(
    #     local_storage_path="data",
    #     servers=servers
    # )
    # file_manager.start_sync_service()

    logger.info(f"âœ… åˆ†å¸ƒå¼HAç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸï¼Œå½“å‰ä¸»èŠ‚ç‚¹: {ha_manager.current_primary}")
    # logger.info(f"âœ… åˆ†å¸ƒå¼æ–‡ä»¶ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸï¼ŒåŒæ­¥åˆ° {len(servers)} ä¸ªæœåŠ¡å™¨")
except Exception as e:
    logger.warning(f"âš ï¸ HAç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ•°æ®åº“: {e}")
    ha_manager = None
    file_manager = None

# åˆ›å»ºä¸€ä¸ª ImageCrawler çš„å•ä¾‹
try:
    # å¦‚æœæœ‰HAç®¡ç†å™¨ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…è£…çš„æ•°æ®åº“ç®¡ç†å™¨
    if ha_manager:
        from database.enhanced_manager import EnhancedDatabaseManager
        # åˆ›å»ºä¸€ä¸ªåŒ…è£…ç®¡ç†å™¨ï¼Œå°†HAç®¡ç†å™¨ä¼ é€’ç»™å®ƒ
        wrapped_db_manager = type('WrappedDBManager', (), {
            'get_session': ha_manager.get_session,
            'create_tables': lambda: None,  # HAç®¡ç†å™¨å·²ç»å¤„ç†äº†è¡¨åˆ›å»º
            'is_disaster_recovery_enabled': lambda: True,
            'get_database_info': lambda: {"type": "HA_PostgreSQL", "status": "active"},
            'get_health_status': lambda: ha_manager.get_cluster_status(),
            'start_monitoring': lambda: None,
            'stop_monitoring': lambda: None,
            'create_backup': lambda name: None,
            'backup_manager': None,
            'get_failover_status': lambda: ha_manager.get_cluster_status(),
            'get_failover_history': lambda limit: []
        })()
        image_crawler = ImageCrawler(db_manager=wrapped_db_manager)
        logger.info("âœ… ä½¿ç”¨HAæ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–çˆ¬è™«")
    else:
        image_crawler = ImageCrawler()
        logger.info("âœ… ä½¿ç”¨é»˜è®¤æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–çˆ¬è™«")

    CRAWLER_ENABLED = True
except Exception as e:
    logger.error(f"âŒ åˆå§‹åŒ– ImageCrawler å¤±è´¥: {e}")
    image_crawler = None
    CRAWLER_ENABLED = False

# çˆ¬è™«ä»»åŠ¡çŠ¶æ€
crawl_status = {
    "is_crawling": False,
    "current_url": None,
    "error": None
}

# --------------------------------------------------------------------------
# Pydantic æ¨¡å‹
# --------------------------------------------------------------------------

class CrawlRequest(BaseModel):
    """çˆ¬å–è¯·æ±‚çš„æ•°æ®æ¨¡å‹"""
    url: str

class DeleteImagesRequest(BaseModel):
    """åˆ é™¤å›¾ç‰‡çš„è¯·æ±‚æ•°æ®æ¨¡å‹"""
    image_ids: List[int]

# --------------------------------------------------------------------------
# åå°ä»»åŠ¡
# --------------------------------------------------------------------------

async def run_crawl_task(url: str):
    """åœ¨åå°è¿è¡Œçˆ¬è™«ä»»åŠ¡"""
    if not CRAWLER_ENABLED:
        logger.error("çˆ¬è™«æœªå¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œä»»åŠ¡")
        crawl_status["error"] = "çˆ¬è™«æœªæ­£ç¡®åˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥é…ç½®"
        return

    crawl_status["is_crawling"] = True
    crawl_status["current_url"] = url
    crawl_status["error"] = None
    
    logger.info(f"ğŸš€ åå°ä»»åŠ¡å¼€å§‹: çˆ¬å– {url}")
    
    try:
        await image_crawler.crawl_website(url=url)
        logger.info(f"âœ… çˆ¬å–å®Œæˆ: {url}")
    except Exception as e:
        logger.error(f"âŒ çˆ¬å–ä»»åŠ¡å¤±è´¥: {url}, é”™è¯¯: {e}")
        crawl_status["error"] = str(e)
    finally:
        crawl_status["is_crawling"] = False
        crawl_status["current_url"] = None
        logger.info("åå°ä»»åŠ¡ç»“æŸ")

# --------------------------------------------------------------------------
# API è·¯ç”±
# --------------------------------------------------------------------------

@app.post("/crawl", status_code=202)
async def start_crawl(request: CrawlRequest, background_tasks: BackgroundTasks):
    """
    å¯åŠ¨ä¸€ä¸ªçˆ¬è™«ä»»åŠ¡
    """
    if not CRAWLER_ENABLED:
        raise HTTPException(status_code=503, detail="çˆ¬è™«æœåŠ¡ä¸å¯ç”¨")

    if crawl_status["is_crawling"]:
        raise HTTPException(status_code=409, detail=f"ä»»åŠ¡è¿›è¡Œä¸­: {crawl_status['current_url']}")
    
    url = request.url
    if not url.startswith(('http://', 'https://')):
        raise HTTPException(status_code=400, detail="æ— æ•ˆçš„ URL")
        
    background_tasks.add_task(run_crawl_task, url)
    
    return {"message": f"çˆ¬å–ä»»åŠ¡å·²å¯åŠ¨: {url}"}


@app.get("/status")
async def get_status():
    """
    è·å–çˆ¬è™«çš„å½“å‰çŠ¶æ€
    """
    return JSONResponse(content={
        "crawling_active": crawl_status["is_crawling"],
        "current_target": crawl_status["current_url"],
        "last_error": crawl_status["error"],
    })

async def log_streamer(request: Request):
    """æµå¼ä¼ è¾“æ—¥å¿—"""
    try:
        while True:
            if await request.is_disconnected():
                break
            log_message = await log_queue.get()
            yield f"data: {log_message}\n\n"
            log_queue.task_done()
    except asyncio.CancelledError:
        logger.info("æ—¥å¿—æµå®¢æˆ·ç«¯æ–­å¼€è¿æ¥")

@app.get("/stream-logs")
async def stream_logs(request: Request):
    return StreamingResponse(log_streamer(request), media_type="text/event-stream")

# --------------------------------------------------------------------------
# æ•°æ®åº“å’Œå›¾ç‰‡æ“ä½œ API
# --------------------------------------------------------------------------

@app.get("/api/images")
async def get_images():
    """
    è·å–æ‰€æœ‰å›¾ç‰‡è®°å½•
    """
    if not CRAWLER_ENABLED:
        raise HTTPException(status_code=503, detail="çˆ¬è™«æœåŠ¡ä¸å¯ç”¨")
    
    try:
        with image_crawler.db_manager.get_session() as session:
            images = session.query(ImageModel).all()
            return [{ "id": img.id, "url": img.url, "file_path": img.local_path, "filename": img.filename } for img in images]
    except Exception as e:
        logger.error(f"è·å–å›¾ç‰‡åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="è·å–å›¾ç‰‡åˆ—è¡¨å¤±è´¥")

@app.delete("/api/images")
async def delete_images(request: DeleteImagesRequest):
    """
    æ‰¹é‡åˆ é™¤å›¾ç‰‡ - ä¼˜åŒ–ç‰ˆæœ¬
    """
    if not CRAWLER_ENABLED:
        raise HTTPException(status_code=503, detail="çˆ¬è™«æœåŠ¡ä¸å¯ç”¨")

    # å¦‚æœå›¾ç‰‡æ•°é‡å¾ˆå¤šï¼Œå¯åŠ¨åå°ä»»åŠ¡
    if len(request.image_ids) > 50:
        task_id = f"delete_task_{int(time.time())}"
        asyncio.create_task(
            _batch_delete_images_background(request.image_ids, task_id)
        )
        return {
            "message": f"å·²å¯åŠ¨åå°åˆ é™¤ä»»åŠ¡ï¼Œå…± {len(request.image_ids)} å¼ å›¾ç‰‡",
            "task_id": task_id,
            "background": True
        }
    else:
        # å°æ‰¹é‡ç›´æ¥å¤„ç†
        return await _batch_delete_images_sync(request.image_ids)

@app.get("/api/db-status")
async def get_db_status():
    """
    è·å–æ•°æ®åº“å¥åº·çŠ¶æ€
    """
    if not CRAWLER_ENABLED:
        return {"status": "unknown", "message": "çˆ¬è™«æœåŠ¡ä¸å¯ç”¨"}

    try:
        if ha_manager:
            # å¦‚æœä½¿ç”¨HAç®¡ç†å™¨ï¼Œè¿”å›é›†ç¾¤çŠ¶æ€
            status = ha_manager.get_cluster_status()
            status["ha_enabled"] = True
            return status
        elif hasattr(image_crawler.db_manager, 'is_disaster_recovery_enabled') and image_crawler.db_manager.is_disaster_recovery_enabled():
            # ä½¿ç”¨é»˜è®¤æ•°æ®åº“ç®¡ç†å™¨çš„å¥åº·ç›‘æ§
            health_status = image_crawler.db_manager.get_health_status()
            health_status["ha_enabled"] = False
            return health_status
        else:
            return {"status": "unknown", "message": "æ•°æ®åº“å¥åº·ç›‘æ§æœªå¯ç”¨", "ha_enabled": False}
    except Exception as e:
        logger.error(f"è·å–æ•°æ®åº“çŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="è·å–æ•°æ®åº“çŠ¶æ€å¤±è´¥")

@app.get("/api/ha-status")
async def get_ha_status():
    """
    è·å–é«˜å¯ç”¨ç³»ç»ŸçŠ¶æ€
    """
    if not ha_manager:
        return {
            "ha_enabled": False,
            "message": "é«˜å¯ç”¨ç³»ç»Ÿæœªå¯ç”¨"
        }

    try:
        status = ha_manager.get_cluster_status()
        status["ha_enabled"] = True
        return status
    except Exception as e:
        logger.error(f"è·å–HAçŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="è·å–HAçŠ¶æ€å¤±è´¥")

@app.get("/api/sync-status")
async def get_sync_status():
    """
    è·å–æ•°æ®åŒæ­¥çŠ¶æ€
    """
    if not ha_manager:
        return {
            "sync_enabled": False,
            "message": "é«˜å¯ç”¨ç³»ç»Ÿæœªå¯ç”¨"
        }

    try:
        sync_status = ha_manager.get_sync_status()
        sync_status["sync_enabled"] = True
        return sync_status
    except Exception as e:
        logger.error(f"è·å–åŒæ­¥çŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="è·å–åŒæ­¥çŠ¶æ€å¤±è´¥")

@app.post("/api/force-sync")
async def force_sync():
    """
    å¼ºåˆ¶æ‰§è¡Œå…¨é‡æ•°æ®åŒæ­¥
    """
    if not ha_manager:
        raise HTTPException(status_code=400, detail="é«˜å¯ç”¨ç³»ç»Ÿæœªå¯ç”¨")

    try:
        success = ha_manager.force_sync_all()
        if success:
            return {
                "status": "success",
                "message": "å…¨é‡åŒæ­¥å·²å¯åŠ¨"
            }
        else:
            return {
                "status": "failed",
                "message": "å…¨é‡åŒæ­¥å¯åŠ¨å¤±è´¥"
            }
    except Exception as e:
        logger.error(f"å¼ºåˆ¶åŒæ­¥å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="å¼ºåˆ¶åŒæ­¥å¤±è´¥")

@app.post("/api/ha-failover/{target_node}")
async def manual_failover(target_node: str):
    """
    æ‰‹åŠ¨æ•…éšœè½¬ç§»
    """
    if not ha_manager:
        raise HTTPException(status_code=503, detail="é«˜å¯ç”¨ç³»ç»Ÿæœªå¯ç”¨")

    try:
        success = ha_manager.manual_failover(target_node)
        if success:
            return {"status": "success", "message": f"æ•…éšœè½¬ç§»åˆ° {target_node} æˆåŠŸ"}
        else:
            raise HTTPException(status_code=400, detail="æ•…éšœè½¬ç§»å¤±è´¥")
    except Exception as e:
        logger.error(f"æ‰‹åŠ¨æ•…éšœè½¬ç§»å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/storage-status")
async def get_storage_status():
    """
    è·å–å­˜å‚¨çŠ¶æ€
    """
    if not file_manager:
        return {
            "distributed_storage": False,
            "message": "åˆ†å¸ƒå¼å­˜å‚¨æœªå¯ç”¨"
        }

    try:
        stats = file_manager.get_storage_stats()
        stats["distributed_storage"] = True
        return stats
    except Exception as e:
        logger.error(f"è·å–å­˜å‚¨çŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="è·å–å­˜å‚¨çŠ¶æ€å¤±è´¥")

@app.post("/api/force-file-sync")
async def force_file_sync():
    """
    å¼ºåˆ¶æ–‡ä»¶åŒæ­¥
    """
    if not file_manager:
        raise HTTPException(status_code=503, detail="åˆ†å¸ƒå¼å­˜å‚¨æœªå¯ç”¨")

    try:
        # æ‰«ææœ¬åœ°æ‰€æœ‰æ–‡ä»¶å¹¶æ·»åŠ åˆ°åŒæ­¥é˜Ÿåˆ—
        sync_count = 0

        for file_path in file_manager.images_path.rglob("*"):
            if file_path.is_file():
                relative_path = file_path.relative_to(file_manager.images_path)
                await file_manager._add_sync_task(str(relative_path))
                sync_count += 1

        return {
            "status": "success",
            "message": f"å·²æ·»åŠ  {sync_count} ä¸ªæ–‡ä»¶åˆ°åŒæ­¥é˜Ÿåˆ—"
        }
    except Exception as e:
        logger.error(f"å¼ºåˆ¶æ–‡ä»¶åŒæ­¥å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# --------------------------------------------------------------------------
# æ–‡ä»¶åŒæ­¥APIé›†æˆ
# --------------------------------------------------------------------------

# å¦‚æœå¯ç”¨äº†åˆ†å¸ƒå¼æ–‡ä»¶ç®¡ç†ï¼Œé›†æˆæ–‡ä»¶åŒæ­¥API
if file_manager:
    try:
        file_sync_api = create_file_sync_api(file_manager)
        app.mount("/file-sync", file_sync_api.app, name="file_sync")
        logger.info("âœ… æ–‡ä»¶åŒæ­¥APIå·²é›†æˆ")
    except Exception as e:
        logger.error(f"âŒ æ–‡ä»¶åŒæ­¥APIé›†æˆå¤±è´¥: {e}")

# --------------------------------------------------------------------------
# é™æ€æ–‡ä»¶æœåŠ¡
# --------------------------------------------------------------------------

app.mount("/static/data", StaticFiles(directory="data"), name="data")
app.mount("/static", StaticFiles(directory="frontend"), name="static")

@app.get("/", include_in_schema=False)
async def read_index():
    index_path = Path("frontend/index.html")
    if not index_path.exists():
        return JSONResponse(status_code=404, content={"message": "index.html not found"})
    return FileResponse(str(index_path))

# --------------------------------------------------------------------------
# ä¸»ç¨‹åºå…¥å£
# --------------------------------------------------------------------------
# æ‰¹é‡åˆ é™¤åŠŸèƒ½
# --------------------------------------------------------------------------

# æ‰¹é‡åˆ é™¤ä»»åŠ¡çŠ¶æ€è·Ÿè¸ª
delete_tasks_status = {}

async def _batch_delete_images_sync(image_ids: list) -> dict:
    """åŒæ­¥æ‰¹é‡åˆ é™¤å›¾ç‰‡ï¼ˆå°æ‰¹é‡ï¼‰"""
    deleted_count = 0
    errors = []

    try:
        with image_crawler.db_manager.get_session() as session:
            # æ‰¹é‡æŸ¥è¯¢å›¾ç‰‡ä¿¡æ¯
            images = session.query(ImageModel).filter(
                ImageModel.id.in_(image_ids)
            ).all()

            # æ”¶é›†æ–‡ä»¶è·¯å¾„
            files_to_delete = []
            images_to_delete = []

            for image in images:
                if image.local_path and os.path.exists(image.local_path):
                    files_to_delete.append(image.local_path)
                images_to_delete.append(image)

            # æ‰¹é‡åˆ é™¤æ–‡ä»¶
            for file_path in files_to_delete:
                try:
                    os.remove(file_path)
                except Exception as e:
                    errors.append(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

            # æ‰¹é‡åˆ é™¤æ•°æ®åº“è®°å½•
            for image in images_to_delete:
                session.delete(image)
                deleted_count += 1

            session.commit()

    except Exception as e:
        logger.error(f"æ‰¹é‡åˆ é™¤å›¾ç‰‡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡åˆ é™¤å¤±è´¥: {e}")

    return {
        "message": f"æˆåŠŸåˆ é™¤ {deleted_count} å¼ å›¾ç‰‡",
        "deleted_count": deleted_count,
        "errors": errors
    }

async def _batch_delete_images_background(image_ids: list, task_id: str):
    """åå°æ‰¹é‡åˆ é™¤å›¾ç‰‡ï¼ˆå¤§æ‰¹é‡ï¼‰"""
    delete_tasks_status[task_id] = {
        "status": "running",
        "total": len(image_ids),
        "processed": 0,
        "deleted": 0,
        "errors": [],
        "start_time": time.time()
    }

    try:
        batch_size = 20  # æ¯æ‰¹å¤„ç†20ä¸ª

        for i in range(0, len(image_ids), batch_size):
            batch_ids = image_ids[i:i + batch_size]

            try:
                with image_crawler.db_manager.get_session() as session:
                    # æ‰¹é‡æŸ¥è¯¢å½“å‰æ‰¹æ¬¡çš„å›¾ç‰‡
                    images = session.query(ImageModel).filter(
                        ImageModel.id.in_(batch_ids)
                    ).all()

                    # åˆ é™¤æ–‡ä»¶å’Œæ•°æ®åº“è®°å½•
                    for image in images:
                        try:
                            # åˆ é™¤æ–‡ä»¶
                            if image.local_path and os.path.exists(image.local_path):
                                os.remove(image.local_path)

                            # åˆ é™¤æ•°æ®åº“è®°å½•
                            session.delete(image)
                            delete_tasks_status[task_id]["deleted"] += 1

                        except Exception as e:
                            delete_tasks_status[task_id]["errors"].append(
                                f"åˆ é™¤å›¾ç‰‡ {image.id} å¤±è´¥: {e}"
                            )

                    session.commit()

            except Exception as e:
                delete_tasks_status[task_id]["errors"].append(
                    f"å¤„ç†æ‰¹æ¬¡å¤±è´¥: {e}"
                )

            delete_tasks_status[task_id]["processed"] += len(batch_ids)

            # çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…è¿‡åº¦å ç”¨èµ„æº
            await asyncio.sleep(0.1)

        delete_tasks_status[task_id]["status"] = "completed"
        delete_tasks_status[task_id]["end_time"] = time.time()

    except Exception as e:
        delete_tasks_status[task_id]["status"] = "failed"
        delete_tasks_status[task_id]["error"] = str(e)
        delete_tasks_status[task_id]["end_time"] = time.time()

@app.get("/api/delete-task/{task_id}")
async def get_delete_task_status(task_id: str):
    """è·å–åˆ é™¤ä»»åŠ¡çŠ¶æ€"""
    if task_id not in delete_tasks_status:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")

    return delete_tasks_status[task_id]

# --------------------------------------------------------------------------

if __name__ == "__main__":
    uvicorn.run(
        "api:app",
        host="127.0.0.1",
        port=8000,
        reload=True,
        log_level="info"
    )
