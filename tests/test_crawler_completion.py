#!/usr/bin/env python3
"""
æµ‹è¯•çˆ¬è™«å®Œæˆæµç¨‹

ä¸“é—¨æµ‹è¯•çˆ¬è™«åœ¨å®Œæˆåæ˜¯å¦èƒ½æ­£ç¡®ä¿å­˜æ•°æ®å’Œæ›´æ–°çŠ¶æ€
"""

import asyncio
import sys
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from crawler.main_crawler import ImageCrawler
from database.enhanced_manager import EnhancedDatabaseManager


async def test_crawler_completion():
    """æµ‹è¯•çˆ¬è™«å®Œæˆæµç¨‹"""
    print("ğŸ§ª æµ‹è¯•çˆ¬è™«å®Œæˆæµç¨‹")
    print("=" * 60)
    
    # åˆ›å»ºæ•°æ®åº“ç®¡ç†å™¨
    db_manager = EnhancedDatabaseManager("sqlite:///test_crawler.db")
    db_manager.create_tables()
    
    # åˆ›å»ºçˆ¬è™«
    crawler = ImageCrawler(db_manager=db_manager)
    
    # æµ‹è¯•URLï¼ˆé€‰æ‹©ä¸€ä¸ªå›¾ç‰‡è¾ƒå°‘çš„ç½‘ç«™è¿›è¡Œå¿«é€Ÿæµ‹è¯•ï¼‰
    test_url = "https://httpbin.org/html"
    
    print(f"ğŸ“¡ å¼€å§‹çˆ¬å–: {test_url}")
    print("â±ï¸ ç›‘æ§çˆ¬å–è¿‡ç¨‹...")
    
    start_time = time.time()
    
    try:
        # æ‰§è¡Œçˆ¬å–
        result = await crawler.crawl_website(
            url=test_url,
            progress_callback=lambda stats: print(f"   è¿›åº¦: {stats}")
        )
        
        end_time = time.time()
        duration = end_time - start_time
        
        print(f"\nâœ… çˆ¬å–å®Œæˆï¼è€—æ—¶: {duration:.2f}ç§’")
        print(f"ğŸ“Š ç»“æœ: {result}")
        
        # æ£€æŸ¥æ•°æ®åº“ä¸­çš„æ•°æ®
        print("\nğŸ” æ£€æŸ¥æ•°æ®åº“çŠ¶æ€...")
        
        with db_manager.get_session() as session:
            from database.models.crawl_session import CrawlSessionModel
            from database.models.image import ImageModel
            
            # æ£€æŸ¥ä¼šè¯çŠ¶æ€
            sessions = session.query(CrawlSessionModel).all()
            print(f"ğŸ“‹ çˆ¬å–ä¼šè¯æ•°é‡: {len(sessions)}")
            
            for sess in sessions:
                print(f"   ä¼šè¯ {sess.id}: {sess.status} - {sess.summary_log}")
            
            # æ£€æŸ¥å›¾ç‰‡æ•°é‡
            images = session.query(ImageModel).all()
            print(f"ğŸ–¼ï¸ å›¾ç‰‡è®°å½•æ•°é‡: {len(images)}")
            
            for img in images[:5]:  # æ˜¾ç¤ºå‰5å¼ å›¾ç‰‡
                print(f"   å›¾ç‰‡: {img.filename} - {img.url}")
        
        return True
        
    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
        return False


async def test_large_crawl_simulation():
    """æ¨¡æ‹Ÿå¤§é‡å›¾ç‰‡çš„çˆ¬å–å®Œæˆæµç¨‹"""
    print("\nğŸ§ª æ¨¡æ‹Ÿå¤§é‡å›¾ç‰‡çˆ¬å–å®Œæˆæµç¨‹")
    print("=" * 60)
    
    # åˆ›å»ºæ•°æ®åº“ç®¡ç†å™¨
    db_manager = EnhancedDatabaseManager("sqlite:///test_large_crawl.db")
    db_manager.create_tables()
    
    # åˆ›å»ºçˆ¬è™«
    crawler = ImageCrawler(db_manager=db_manager)
    
    # æ¨¡æ‹Ÿå¤§é‡å›¾ç‰‡çš„ç»“æœ
    mock_result = {
        'success': True,
        'start_url': 'https://example.com/gallery',
        'summary': 'é¡µé¢: 100, å›¾ç‰‡: 500/600, è€—æ—¶: 45.00ç§’',
        'downloaded_images': [
            f'https://example.com/image_{i}.jpg' for i in range(500)
        ],
        'stats': {
            'pages_crawled': 100,
            'images_found': 600,
            'images_downloaded': 500,
            'images_failed': 100,
            'total_time': 45.0
        }
    }
    
    print(f"ğŸ“Š æ¨¡æ‹Ÿç»“æœ: {len(mock_result['downloaded_images'])} å¼ å›¾ç‰‡")
    
    try:
        # åˆ›å»ºä¼šè¯
        session_id = await crawler._create_crawl_session(
            'https://example.com/gallery',
            'large_test'
        )
        
        print(f"ğŸ“ åˆ›å»ºä¼šè¯: {session_id}")
        
        # æµ‹è¯•ä¿å­˜ç»“æœ
        print("ğŸ’¾ æµ‹è¯•ä¿å­˜ç»“æœ...")
        start_time = time.time()
        
        await crawler._save_crawl_results(session_id, mock_result)
        
        save_time = time.time() - start_time
        print(f"âœ… ä¿å­˜å®Œæˆï¼Œè€—æ—¶: {save_time:.2f}ç§’")
        
        # æµ‹è¯•å®Œæˆä¼šè¯
        print("ğŸ æµ‹è¯•å®Œæˆä¼šè¯...")
        start_time = time.time()
        
        await crawler._complete_crawl_session(session_id, mock_result)
        
        complete_time = time.time() - start_time
        print(f"âœ… ä¼šè¯å®Œæˆï¼Œè€—æ—¶: {complete_time:.2f}ç§’")
        
        # éªŒè¯æ•°æ®åº“çŠ¶æ€
        print("\nğŸ” éªŒè¯æ•°æ®åº“çŠ¶æ€...")
        
        with db_manager.get_session() as session:
            from database.models.crawl_session import CrawlSessionModel
            from database.models.image import ImageModel
            
            # æ£€æŸ¥ä¼šè¯çŠ¶æ€
            crawl_session = session.query(CrawlSessionModel).filter(
                CrawlSessionModel.id == session_id
            ).first()
            
            if crawl_session:
                print(f"ğŸ“‹ ä¼šè¯çŠ¶æ€: {crawl_session.status}")
                print(f"ğŸ“ ä¼šè¯æ—¥å¿—: {crawl_session.summary_log}")
            
            # æ£€æŸ¥å›¾ç‰‡æ•°é‡
            image_count = session.query(ImageModel).count()
            print(f"ğŸ–¼ï¸ æ•°æ®åº“ä¸­çš„å›¾ç‰‡æ•°é‡: {image_count}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª çˆ¬è™«å®Œæˆæµç¨‹æµ‹è¯•")
    print("=" * 80)
    
    # æµ‹è¯•1: å°è§„æ¨¡çˆ¬å–
    success1 = await test_crawler_completion()
    
    # æµ‹è¯•2: å¤§è§„æ¨¡æ•°æ®ä¿å­˜æ¨¡æ‹Ÿ
    success2 = await test_large_crawl_simulation()
    
    print("\nğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"   å°è§„æ¨¡çˆ¬å–: {'âœ… é€šè¿‡' if success1 else 'âŒ å¤±è´¥'}")
    print(f"   å¤§è§„æ¨¡ä¿å­˜: {'âœ… é€šè¿‡' if success2 else 'âŒ å¤±è´¥'}")
    
    if success1 and success2:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼çˆ¬è™«å®Œæˆæµç¨‹æ­£å¸¸å·¥ä½œã€‚")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")
    
    print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    print("1. ä½¿ç”¨æ‰¹é‡æ•°æ®åº“æ“ä½œå‡å°‘ä¿å­˜æ—¶é—´")
    print("2. æ·»åŠ è¶…æ—¶æœºåˆ¶é˜²æ­¢å¡æ­»")
    print("3. å¢åŠ é”™è¯¯å¤„ç†ç¡®ä¿æµç¨‹å®Œæ•´æ€§")


if __name__ == "__main__":
    asyncio.run(main())
