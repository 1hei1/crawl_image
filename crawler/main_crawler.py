"""
ä¸»çˆ¬è™«ç±»

æ•´åˆæ‰€æœ‰çˆ¬è™«åŠŸèƒ½çš„ä¸»è¦æ¥å£
"""

import asyncio
import logging
from typing import Dict, Any, Optional, Callable, List
from pathlib import Path
import time
from datetime import datetime, timezone

from crawler.core.async_crawler import AsyncCrawler, TaskScheduler
from crawler.core.spider import ImageSpider
from crawler.core.downloader import ImageDownloader
from database.enhanced_manager import EnhancedDatabaseManager
from database.models.image import ImageModel
from database.models.crawl_session import CrawlSessionModel
from config.manager import ConfigManager

logger = logging.getLogger(__name__)


class ImageCrawler:
    """
    ä¸»å›¾ç‰‡çˆ¬è™«ç±»
    
    åŠŸèƒ½ï¼š
    - ç»Ÿä¸€çš„çˆ¬è™«æ¥å£
    - æ•°æ®åº“é›†æˆ
    - é…ç½®ç®¡ç†
    - è¿›åº¦è·Ÿè¸ª
    - ç»“æœå­˜å‚¨
    """
    
    def __init__(self, config_file: Optional[str] = None, db_manager=None):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
            db_manager: å¤–éƒ¨æ•°æ®åº“ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼Œç”¨äºHAç³»ç»Ÿï¼‰
        """
        # åŠ è½½é…ç½®
        self.config_manager = ConfigManager(config_file)
        self.settings = self.config_manager.get_settings()

        # ä½¿ç”¨å¤–éƒ¨æ•°æ®åº“ç®¡ç†å™¨æˆ–åˆ›å»ºé»˜è®¤ç®¡ç†å™¨
        if db_manager is not None:
            self.db_manager = db_manager
            logger.info("âœ… ä½¿ç”¨å¤–éƒ¨æ•°æ®åº“ç®¡ç†å™¨ï¼ˆHAç³»ç»Ÿï¼‰")
        else:
            # åˆå§‹åŒ–å¢å¼ºæ•°æ®åº“ç®¡ç†å™¨ï¼ˆæ”¯æŒå®¹ç¾å¤‡ä»½ï¼‰
            database_url = self.config_manager.get_database_url()
            self.db_manager = EnhancedDatabaseManager(
                database_url,
                self.settings.disaster_recovery
            )
            logger.info("âœ… ä½¿ç”¨é»˜è®¤æ•°æ®åº“ç®¡ç†å™¨")

        # åˆ›å»ºæ•°æ®åº“è¡¨ï¼ˆå¦‚æœä½¿ç”¨é»˜è®¤ç®¡ç†å™¨ï¼‰
        if db_manager is None:
            self.db_manager.create_tables()
        
        # ä»»åŠ¡è°ƒåº¦å™¨
        self.task_scheduler = TaskScheduler(
            max_concurrent_crawlers=self.settings.crawler.max_concurrent
        )
        
        # å½“å‰ä¼šè¯
        self.current_session: Optional[CrawlSessionModel] = None

        # å¯åŠ¨å®¹ç¾å¤‡ä»½ç›‘æ§ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self._start_disaster_recovery()

        logger.info("å›¾ç‰‡çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
    
    async def crawl_website(self, 
                           url: str, 
                           session_name: Optional[str] = None,
                           progress_callback: Optional[Callable] = None) -> Dict[str, Any]:
        """
        çˆ¬å–ç½‘ç«™å›¾ç‰‡
        
        Args:
            url: ç›®æ ‡ç½‘ç«™URL
            session_name: ä¼šè¯åç§°
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            çˆ¬å–ç»“æœ
        """
        # åˆ›å»ºçˆ¬å–ä¼šè¯
        session_id = await self._create_crawl_session(url, session_name)
        
        try:
            # å‡†å¤‡é…ç½®
            crawler_config = {
                'max_concurrent': self.settings.crawler.max_concurrent,
                'max_depth': self.settings.crawler.max_depth,
                'max_images': self.settings.crawler.max_images,
                'max_pages': 100,  # å¯ä»¥ä»é…ç½®ä¸­è·å–
                'download_path': self.settings.crawler.download_path,
                'anti_crawler': {
                    'use_random_user_agent': self.settings.anti_crawler.use_random_user_agent,
                    'default_headers': self.settings.anti_crawler.default_headers,
                    'use_proxy': self.settings.anti_crawler.use_proxy,
                    'proxy_list': self.settings.anti_crawler.proxy_list,
                    'random_delay': self.settings.anti_crawler.random_delay,
                    'min_delay': self.settings.anti_crawler.min_delay,
                    'max_delay': self.settings.anti_crawler.max_delay,
                }
            }
            
            # åˆ›å»ºè¿›åº¦å›è°ƒåŒ…è£…å™¨
            async def wrapped_progress_callback(stats):
                await self._update_session_progress(session_id, stats)
                if progress_callback:
                    await progress_callback(stats)
            
            # æ‰§è¡Œçˆ¬å–
            result = await self.task_scheduler.schedule_crawl(
                task_id=f"session_{session_id}",
                start_url=url,
                config=crawler_config
            )
            
            # ä¿å­˜ç»“æœåˆ°æ•°æ®åº“ï¼ˆæ·»åŠ è¶…æ—¶ï¼‰
            try:
                await asyncio.wait_for(
                    self._save_crawl_results(session_id, result),
                    timeout=60.0  # 60ç§’è¶…æ—¶
                )
            except asyncio.TimeoutError:
                logger.error(f"ä¿å­˜çˆ¬å–ç»“æœè¶…æ—¶: {session_id}")
            except Exception as e:
                logger.error(f"ä¿å­˜çˆ¬å–ç»“æœå¤±è´¥: {session_id} -> {e}")

            # æ ‡è®°ä¼šè¯å®Œæˆï¼ˆæ·»åŠ è¶…æ—¶ï¼‰
            try:
                await asyncio.wait_for(
                    self._complete_crawl_session(session_id, result),
                    timeout=30.0  # 30ç§’è¶…æ—¶
                )
            except asyncio.TimeoutError:
                logger.error(f"å®Œæˆçˆ¬å–ä¼šè¯è¶…æ—¶: {session_id}")
            except Exception as e:
                logger.error(f"å®Œæˆçˆ¬å–ä¼šè¯å¤±è´¥: {session_id} -> {e}")
            
            return result
            
        except Exception as e:
            logger.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            await self._fail_crawl_session(session_id, str(e))
            raise
    
    async def crawl_multiple_websites(self, 
                                    urls: List[str],
                                    session_name: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡çˆ¬å–å¤šä¸ªç½‘ç«™
        
        Args:
            urls: URLåˆ—è¡¨
            session_name: ä¼šè¯åç§°å‰ç¼€
            
        Returns:
            çˆ¬å–ç»“æœåˆ—è¡¨
        """
        tasks = []
        for i, url in enumerate(urls):
            task_name = f"{session_name}_{i}" if session_name else f"batch_{i}"
            
            crawler_config = {
                'max_concurrent': self.settings.crawler.max_concurrent,
                'max_depth': self.settings.crawler.max_depth,
                'max_images': self.settings.crawler.max_images,
                'download_path': self.settings.crawler.download_path,
                'anti_crawler': {
                    'use_random_user_agent': self.settings.anti_crawler.use_random_user_agent,
                    'default_headers': self.settings.anti_crawler.default_headers,
                    'use_proxy': self.settings.anti_crawler.use_proxy,
                    'proxy_list': self.settings.anti_crawler.proxy_list,
                    'random_delay': self.settings.anti_crawler.random_delay,
                    'min_delay': self.settings.anti_crawler.min_delay,
                    'max_delay': self.settings.anti_crawler.max_delay,
                }
            }
            
            tasks.append({
                'task_id': task_name,
                'start_url': url,
                'config': crawler_config
            })
        
        # æ‰§è¡Œæ‰¹é‡çˆ¬å–
        results = await self.task_scheduler.schedule_multiple_crawls(tasks)
        
        return results
    
    async def _create_crawl_session(self, target_url: str, session_name: Optional[str] = None) -> int:
        """åˆ›å»ºçˆ¬å–ä¼šè¯"""
        with self.db_manager.get_session() as db_session:
            crawl_session = CrawlSessionModel(
                session_name=session_name or f"crawl_{int(time.time())}",
                target_url=target_url,
                status="running",
                start_time=datetime.now(timezone.utc),
                config_data={
                    'max_depth': self.settings.crawler.max_depth,
                    'max_images': self.settings.crawler.max_images,
                    'max_concurrent': self.settings.crawler.max_concurrent,
                }
            )
            
            db_session.add(crawl_session)
            db_session.commit()
            
            self.current_session = crawl_session
            logger.info(f"åˆ›å»ºçˆ¬å–ä¼šè¯: {crawl_session.id}")
            
            return crawl_session.id
    
    async def _update_session_progress(self, session_id: int, stats: Dict[str, Any]):
        """æ›´æ–°ä¼šè¯è¿›åº¦"""
        with self.db_manager.get_session() as db_session:
            session = db_session.query(CrawlSessionModel).filter(
                CrawlSessionModel.id == session_id
            ).first()
            
            if session:
                session.processed_pages = stats.get('pages_crawled', 0)
                session.total_images_found = stats.get('images_found', 0)
                session.images_downloaded = stats.get('images_downloaded', 0)
                session.images_failed = stats.get('images_failed', 0)
                
                db_session.commit()
    
    async def _save_crawl_results(self, session_id: int, result: Dict[str, Any]):
        """ä¿å­˜çˆ¬å–ç»“æœåˆ°æ•°æ®åº“"""
        if not result.get('success', False):
            return

        downloaded_images = result.get('downloaded_images', [])
        if not downloaded_images:
            logger.info("æ²¡æœ‰ä¸‹è½½çš„å›¾ç‰‡éœ€è¦ä¿å­˜")
            return

        logger.info(f"å¼€å§‹ä¿å­˜ {len(downloaded_images)} å¼ å›¾ç‰‡ä¿¡æ¯åˆ°æ•°æ®åº“...")

        try:
            with self.db_manager.get_session() as db_session:
                # è·å–é»˜è®¤åˆ†ç±»
                from database.models.category import CategoryModel
                default_category = db_session.query(CategoryModel).filter(
                    CategoryModel.slug == "uncategorized"
                ).first()

                # æ‰¹é‡æ£€æŸ¥å·²å­˜åœ¨çš„å›¾ç‰‡
                existing_urls = set()
                if downloaded_images:
                    existing_images = db_session.query(ImageModel.url).filter(
                        ImageModel.url.in_(downloaded_images)
                    ).all()
                    existing_urls = {img.url for img in existing_images}

                # è·å–URLåˆ°æ–‡ä»¶åçš„æ˜ å°„
                url_to_filename = result.get('url_to_filename', {})

                # æ‰¹é‡åˆ›å»ºæ–°å›¾ç‰‡è®°å½•
                new_images = []
                for image_url in downloaded_images:
                    if image_url not in existing_urls:
                        # ä½¿ç”¨å®é™…çš„æ–‡ä»¶åï¼Œå¦‚æœæ²¡æœ‰æ˜ å°„åˆ™ä»URLæå–
                        actual_filename = url_to_filename.get(image_url)
                        if actual_filename:
                            filename = actual_filename
                            file_extension = Path(actual_filename).suffix or '.jpg'
                        else:
                            # å›é€€åˆ°ä»URLæå–ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
                            filename = Path(image_url).name
                            file_extension = Path(image_url).suffix or '.jpg'

                        image = ImageModel(
                            url=image_url,
                            source_url=result.get('start_url', ''),
                            filename=filename,
                            file_extension=file_extension,
                            category_id=default_category.id if default_category else None,
                            is_downloaded=True,
                        )
                        new_images.append(image)

                # æ‰¹é‡æ·»åŠ åˆ°æ•°æ®åº“
                if new_images:
                    db_session.add_all(new_images)
                    db_session.commit()
                    logger.info(f"ä¿å­˜äº† {len(new_images)} å¼ æ–°å›¾ç‰‡ä¿¡æ¯åˆ°æ•°æ®åº“")
                else:
                    logger.info("æ‰€æœ‰å›¾ç‰‡éƒ½å·²å­˜åœ¨ï¼Œæ— éœ€ä¿å­˜")

        except Exception as e:
            logger.error(f"ä¿å­˜å›¾ç‰‡ä¿¡æ¯åˆ°æ•°æ®åº“å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“ä¼šè¯å®Œæˆ
    
    async def _complete_crawl_session(self, session_id: int, result: Dict[str, Any]):
        """å®Œæˆçˆ¬å–ä¼šè¯"""
        try:
            logger.info(f"æ­£åœ¨å®Œæˆçˆ¬å–ä¼šè¯: {session_id}")

            with self.db_manager.get_session() as db_session:
                session = db_session.query(CrawlSessionModel).filter(
                    CrawlSessionModel.id == session_id
                ).first()

                if session:
                    session.mark_completed()
                    session.summary_log = f"çˆ¬å–å®Œæˆ: {result.get('summary', '')}"

                    db_session.commit()
                    logger.info(f"çˆ¬å–ä¼šè¯å®Œæˆ: {session_id}")
                else:
                    logger.warning(f"æœªæ‰¾åˆ°ä¼šè¯: {session_id}")

        except Exception as e:
            logger.error(f"å®Œæˆçˆ¬å–ä¼šè¯å¤±è´¥: {session_id} -> {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“æ•´ä¸ªçˆ¬å–æµç¨‹
    
    async def _fail_crawl_session(self, session_id: int, error_message: str):
        """æ ‡è®°çˆ¬å–ä¼šè¯å¤±è´¥"""
        with self.db_manager.get_session() as db_session:
            session = db_session.query(CrawlSessionModel).filter(
                CrawlSessionModel.id == session_id
            ).first()
            
            if session:
                session.mark_failed(error_message)
                db_session.commit()
                logger.error(f"çˆ¬å–ä¼šè¯å¤±è´¥: {session_id} -> {error_message}")
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯"""
        with self.db_manager.get_session() as db_session:
            total_images = db_session.query(ImageModel).count()
            downloaded_images = db_session.query(ImageModel).filter(
                ImageModel.is_downloaded == True
            ).count()
            total_sessions = db_session.query(CrawlSessionModel).count()
            
            return {
                'total_images': total_images,
                'downloaded_images': downloaded_images,
                'total_sessions': total_sessions,
                'active_tasks': len(self.task_scheduler.get_active_tasks()),
                'database_info': self.db_manager.get_database_info(),
            }

    def _start_disaster_recovery(self):
        """å¯åŠ¨å®¹ç¾å¤‡ä»½åŠŸèƒ½"""
        try:
            if self.db_manager.is_disaster_recovery_enabled():
                logger.info("ğŸ›¡ï¸ å¯åŠ¨å®¹ç¾å¤‡ä»½ç›‘æ§...")

                # å¯åŠ¨ç›‘æ§æœåŠ¡
                try:
                    self.db_manager.start_monitoring()
                    logger.info("âœ… ç›‘æ§æœåŠ¡å¯åŠ¨æˆåŠŸ")
                except Exception as e:
                    logger.error(f"âŒ å¯åŠ¨ç›‘æ§æœåŠ¡å¤±è´¥: {e}")

                # åˆ›å»ºåˆå§‹å¤‡ä»½ï¼ˆå¯é€‰ï¼Œå¦‚æœå¤±è´¥ä¸å½±å“ç³»ç»Ÿå¯åŠ¨ï¼‰
                try:
                    backup_path = self.db_manager.create_backup("initial_backup")
                    if backup_path:
                        logger.info(f"âœ… åˆå§‹å¤‡ä»½åˆ›å»ºæˆåŠŸ: {backup_path}")
                    else:
                        logger.warning("âš ï¸ åˆå§‹å¤‡ä»½åˆ›å»ºå¤±è´¥ï¼Œä½†ç³»ç»Ÿå°†ç»§ç»­è¿è¡Œ")
                except Exception as e:
                    logger.warning(f"âš ï¸ åˆ›å»ºåˆå§‹å¤‡ä»½å¤±è´¥: {e}ï¼Œä½†ç³»ç»Ÿå°†ç»§ç»­è¿è¡Œ")

                # ç¡®ä¿å¤‡ç”¨æ•°æ®åº“æœ‰è¡¨ç»“æ„ï¼Œç„¶ååŒæ­¥æ•°æ®ï¼ˆå¯é€‰ï¼Œå¦‚æœå¤±è´¥ä¸å½±å“ç³»ç»Ÿå¯åŠ¨ï¼‰
                try:
                    self._ensure_backup_database_schema()
                    self._sync_to_backup_databases()
                except Exception as e:
                    logger.warning(f"âš ï¸ æ•°æ®åŒæ­¥å¤±è´¥: {e}ï¼Œä½†ç³»ç»Ÿå°†ç»§ç»­è¿è¡Œ")

                logger.info("âœ… å®¹ç¾å¤‡ä»½ç³»ç»Ÿå·²å¯åŠ¨")
            else:
                logger.info("â„¹ï¸ å®¹ç¾å¤‡ä»½åŠŸèƒ½æœªå¯ç”¨")

        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨å®¹ç¾å¤‡ä»½å¤±è´¥: {e}ï¼Œä½†ç³»ç»Ÿå°†ç»§ç»­è¿è¡Œ")

    def _sync_to_backup_databases(self):
        """åŒæ­¥æ•°æ®åˆ°å¤‡ç”¨æ•°æ®åº“"""
        try:
            if not self.db_manager.backup_manager:
                return

            backup_manager = self.db_manager.backup_manager
            primary_db = backup_manager.current_primary

            if not primary_db:
                return

            # æ£€æŸ¥ä¸»æ•°æ®åº“æ˜¯å¦æœ‰æ•°æ®
            try:
                with backup_manager.get_session(primary_db) as session:
                    from database.models.image import ImageModel
                    image_count = session.query(ImageModel).count()

                    if image_count == 0:
                        logger.info("â„¹ï¸ ä¸»æ•°æ®åº“ä¸ºç©ºï¼Œè·³è¿‡æ•°æ®åŒæ­¥")
                        return

                    logger.info(f"ğŸ“Š ä¸»æ•°æ®åº“åŒ…å« {image_count} æ¡å›¾ç‰‡è®°å½•")
            except Exception as e:
                logger.warning(f"æ£€æŸ¥ä¸»æ•°æ®åº“æ•°æ®æ—¶å‡ºé”™: {e}")
                return

            # è·å–æ‰€æœ‰å¤‡ç”¨æ•°æ®åº“
            backup_dbs = [
                name for name, config in backup_manager.databases.items()
                if name != primary_db and config.type == 'secondary' and config.is_active
            ]

            if backup_dbs:
                logger.info(f"ğŸ”„ å¼€å§‹åŒæ­¥æ•°æ®åˆ° {len(backup_dbs)} ä¸ªå¤‡ç”¨æ•°æ®åº“...")

                sync_success_count = 0
                for backup_db in backup_dbs:
                    try:
                        # æ£€æŸ¥å¤‡ç”¨æ•°æ®åº“æ˜¯å¦å·²æœ‰æ•°æ®
                        try:
                            with backup_manager.get_session(backup_db) as session:
                                from database.models.image import ImageModel
                                backup_image_count = session.query(ImageModel).count()

                                if backup_image_count >= image_count:
                                    logger.info(f"â„¹ï¸ å¤‡ç”¨æ•°æ®åº“ {backup_db} æ•°æ®å·²æ˜¯æœ€æ–°ï¼Œè·³è¿‡åŒæ­¥")
                                    sync_success_count += 1
                                    continue
                        except Exception:
                            # å¦‚æœæ— æ³•æŸ¥è¯¢å¤‡ç”¨æ•°æ®åº“ï¼Œç»§ç»­å°è¯•åŒæ­¥
                            pass

                        logger.info(f"ğŸ”„ æ­£åœ¨åŒæ­¥åˆ° {backup_db}...")
                        success = backup_manager.sync_databases(primary_db, backup_db)
                        if success:
                            logger.info(f"âœ… æ•°æ®åŒæ­¥æˆåŠŸ: {primary_db} -> {backup_db}")
                            sync_success_count += 1
                        else:
                            logger.warning(f"âš ï¸ æ•°æ®åŒæ­¥å¤±è´¥: {primary_db} -> {backup_db}")
                    except Exception as e:
                        logger.error(f"âŒ åŒæ­¥åˆ° {backup_db} å¤±è´¥: {e}")

                logger.info(f"ğŸ”„ æ•°æ®åŒæ­¥å®Œæˆï¼ŒæˆåŠŸåŒæ­¥åˆ° {sync_success_count}/{len(backup_dbs)} ä¸ªå¤‡ç”¨æ•°æ®åº“")
            else:
                logger.info("â„¹ï¸ æ²¡æœ‰å¯ç”¨çš„å¤‡ç”¨æ•°æ®åº“è¿›è¡ŒåŒæ­¥")

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åŒæ­¥å¤±è´¥: {e}")

    def _ensure_backup_database_schema(self):
        """ç¡®ä¿å¤‡ç”¨æ•°æ®åº“æœ‰æ­£ç¡®çš„è¡¨ç»“æ„"""
        try:
            if not self.db_manager.backup_manager:
                return

            backup_manager = self.db_manager.backup_manager
            primary_db = backup_manager.current_primary

            if not primary_db:
                return

            # è·å–æ‰€æœ‰å¤‡ç”¨æ•°æ®åº“
            backup_dbs = [
                name for name, config in backup_manager.databases.items()
                if name != primary_db and config.type == 'secondary' and config.is_active
            ]

            if backup_dbs:
                logger.info(f"ğŸ”§ æ£€æŸ¥ {len(backup_dbs)} ä¸ªå¤‡ç”¨æ•°æ®åº“çš„è¡¨ç»“æ„...")

                for backup_db in backup_dbs:
                    try:
                        engine = backup_manager.engines[backup_db]

                        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                        from sqlalchemy import inspect
                        inspector = inspect(engine)
                        tables = inspector.get_table_names()

                        required_tables = ['images', 'categories', 'crawl_sessions', 'tags']
                        missing_tables = [table for table in required_tables if table not in tables]

                        if missing_tables:
                            logger.info(f"ğŸ”§ åœ¨ {backup_db} ä¸­åˆ›å»ºç¼ºå°‘çš„è¡¨: {missing_tables}")

                            # åˆ›å»ºç¼ºå°‘çš„è¡¨
                            from database.models.base import Base
                            Base.metadata.create_all(bind=engine)

                            # éªŒè¯è¡¨æ˜¯å¦åˆ›å»ºæˆåŠŸ
                            inspector = inspect(engine)
                            tables = inspector.get_table_names()
                            still_missing = [table for table in required_tables if table not in tables]

                            if still_missing:
                                logger.error(f"âŒ åœ¨ {backup_db} ä¸­åˆ›å»ºè¡¨å¤±è´¥ï¼Œä»ç¼ºå°‘: {still_missing}")
                            else:
                                logger.info(f"âœ… åœ¨ {backup_db} ä¸­æˆåŠŸåˆ›å»ºæ‰€æœ‰å¿…è¦çš„è¡¨")
                        else:
                            logger.info(f"âœ… {backup_db} çš„è¡¨ç»“æ„å®Œæ•´")

                    except Exception as e:
                        logger.error(f"âŒ æ£€æŸ¥ {backup_db} è¡¨ç»“æ„å¤±è´¥: {e}")

                logger.info("ğŸ”§ å¤‡ç”¨æ•°æ®åº“è¡¨ç»“æ„æ£€æŸ¥å®Œæˆ")
            else:
                logger.info("â„¹ï¸ æ²¡æœ‰å¯ç”¨çš„å¤‡ç”¨æ•°æ®åº“éœ€è¦æ£€æŸ¥è¡¨ç»“æ„")

        except Exception as e:
            logger.error(f"âŒ ç¡®ä¿å¤‡ç”¨æ•°æ®åº“è¡¨ç»“æ„å¤±è´¥: {e}")

    def stop_disaster_recovery(self):
        """åœæ­¢å®¹ç¾å¤‡ä»½åŠŸèƒ½"""
        try:
            if self.db_manager.is_disaster_recovery_enabled():
                logger.info("ğŸ›¡ï¸ åœæ­¢å®¹ç¾å¤‡ä»½ç›‘æ§...")
                self.db_manager.stop_monitoring()
                logger.info("âœ… å®¹ç¾å¤‡ä»½ç³»ç»Ÿå·²åœæ­¢")
        except Exception as e:
            logger.error(f"âŒ åœæ­¢å®¹ç¾å¤‡ä»½å¤±è´¥: {e}")

    def get_disaster_recovery_status(self) -> Dict[str, Any]:
        """è·å–å®¹ç¾å¤‡ä»½çŠ¶æ€"""
        try:
            if not self.db_manager.is_disaster_recovery_enabled():
                return {"enabled": False, "message": "å®¹ç¾å¤‡ä»½åŠŸèƒ½æœªå¯ç”¨"}

            return {
                "enabled": True,
                "database_status": self.db_manager.get_database_info(),
                "health_status": self.db_manager.get_health_status(),
                "failover_status": self.db_manager.get_failover_status(),
                "failover_history": self.db_manager.get_failover_history(5)
            }
        except Exception as e:
            logger.error(f"è·å–å®¹ç¾çŠ¶æ€å¤±è´¥: {e}")
            return {"enabled": False, "error": str(e)}

    def stop_all_tasks(self):
        """åœæ­¢æ‰€æœ‰æ´»è·ƒä»»åŠ¡"""
        self.task_scheduler.stop_all_tasks()
