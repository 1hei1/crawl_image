#!/usr/bin/env python3
"""
å›¾ç‰‡çˆ¬è™«å¿«é€Ÿå¯åŠ¨è„šæœ¬

æä¾›ç®€å•çš„äº¤äº’å¼ç•Œé¢
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from crawler.main_crawler import ImageCrawler
from crawler.utils.logger import LoggerManager


def print_banner():
    """æ‰“å°ç¨‹åºæ¨ªå¹…"""
    banner = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    æ™ºèƒ½å›¾ç‰‡çˆ¬è™«ç³»ç»Ÿ                          â•‘
â•‘                  Image Crawler v1.0.0                       â•‘
â•‘                                                              â•‘
â•‘  åŠŸèƒ½ç‰¹æ€§:                                                   â•‘
â•‘  â€¢ æ™ºèƒ½å›¾ç‰‡å‘ç°å’Œä¸‹è½½                                       â•‘
â•‘  â€¢ åçˆ¬è™«æœºåˆ¶å¤„ç†                                           â•‘
â•‘  â€¢ å¼‚æ­¥é«˜å¹¶å‘å¤„ç†                                           â•‘
â•‘  â€¢ å›¾ç‰‡æ™ºèƒ½åˆ†ç±»                                             â•‘
â•‘  â€¢ å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—                                     â•‘
â•‘  â€¢ æ•°æ®åº“å®¹ç¾å¤‡ä»½                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    print(banner)


def print_menu():
    """æ‰“å°ä¸»èœå•"""
    menu = """
è¯·é€‰æ‹©æ“ä½œ:
1. çˆ¬å–å•ä¸ªç½‘ç«™
2. æ‰¹é‡çˆ¬å–ç½‘ç«™
3. æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
4. å®¹ç¾å¤‡ä»½ç®¡ç†
5. æ¸…ç†æ•°æ®
6. ç”Ÿæˆé…ç½®æ–‡ä»¶
7. æµ‹è¯•ç³»ç»Ÿ
8. é€€å‡º

è¯·è¾“å…¥é€‰é¡¹ (1-8): """
    return input(menu).strip()


async def crawl_single_website(crawler: ImageCrawler):
    """çˆ¬å–å•ä¸ªç½‘ç«™"""
    url = input("è¯·è¾“å…¥è¦çˆ¬å–çš„ç½‘ç«™URL: ").strip()
    if not url:
        print("âŒ URLä¸èƒ½ä¸ºç©º")
        return
    
    session_name = input("è¯·è¾“å…¥ä¼šè¯åç§° (å¯é€‰): ").strip() or None
    
    print(f"\nğŸš€ å¼€å§‹çˆ¬å–: {url}")
    print("æŒ‰ Ctrl+C å¯ä»¥ä¸­æ–­çˆ¬å–\n")
    
    # è¿›åº¦å›è°ƒ
    import time
    last_update_time = [0]

    async def progress_callback(stats):
        current_time = time.time()
        # æ¯2ç§’æ›´æ–°ä¸€æ¬¡è¿›åº¦ï¼Œé¿å…åˆ·å±
        if current_time - last_update_time[0] >= 2:
            print(f"\rğŸ”„ é¡µé¢: {stats.get('pages_crawled', 0)} | "
                  f"å‘ç°: {stats.get('images_found', 0)} | "
                  f"ä¸‹è½½: {stats.get('images_downloaded', 0)} | "
                  f"å¤±è´¥: {stats.get('images_failed', 0)}", end='', flush=True)
            last_update_time[0] = current_time
    
    try:
        result = await crawler.crawl_website(
            url=url,
            session_name=session_name,
            progress_callback=progress_callback
        )
        
        print("\n")  # æ¢è¡Œ
        
        if result.get('success', False):
            print("âœ… çˆ¬å–å®Œæˆ!")
            print(f"ğŸ“Š {result.get('summary', '')}")
            
            stats = result.get('stats', {})
            print(f"\nè¯¦ç»†ç»Ÿè®¡:")
            print(f"  â€¢ çˆ¬å–é¡µé¢: {stats.get('pages_crawled', 0)}")
            print(f"  â€¢ å‘ç°å›¾ç‰‡: {stats.get('images_found', 0)}")
            print(f"  â€¢ ä¸‹è½½æˆåŠŸ: {stats.get('images_downloaded', 0)}")
            print(f"  â€¢ ä¸‹è½½å¤±è´¥: {stats.get('images_failed', 0)}")
            print(f"  â€¢ è€—æ—¶: {stats.get('duration', 0):.2f}ç§’")
            print(f"  â€¢ æˆåŠŸç‡: {stats.get('success_rate', 0):.1f}%")
        else:
            print(f"âŒ çˆ¬å–å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        crawler.stop_all_tasks()
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")


async def batch_crawl_websites(crawler: ImageCrawler):
    """æ‰¹é‡çˆ¬å–ç½‘ç«™"""
    print("è¯·è¾“å…¥è¦çˆ¬å–çš„ç½‘ç«™URL (æ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸ):")
    
    urls = []
    while True:
        url = input().strip()
        if not url:
            break
        urls.append(url)
    
    if not urls:
        print("âŒ æ²¡æœ‰è¾“å…¥ä»»ä½•URL")
        return
    
    session_name = input("è¯·è¾“å…¥æ‰¹æ¬¡åç§°å‰ç¼€ (å¯é€‰): ").strip() or None
    
    print(f"\nğŸš€ å¼€å§‹æ‰¹é‡çˆ¬å– {len(urls)} ä¸ªç½‘ç«™")
    print("æŒ‰ Ctrl+C å¯ä»¥ä¸­æ–­çˆ¬å–\n")
    
    try:
        results = await crawler.crawl_multiple_websites(urls, session_name)
        
        # ç»Ÿè®¡ç»“æœ
        successful = sum(1 for r in results if r.get('success', False))
        failed = len(results) - successful
        
        print(f"\nğŸ“Š æ‰¹é‡çˆ¬å–å®Œæˆ:")
        print(f"  â€¢ æˆåŠŸ: {successful}")
        print(f"  â€¢ å¤±è´¥: {failed}")
        print(f"  â€¢ æ€»è®¡: {len(results)}")
        
        # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
        print(f"\nè¯¦ç»†ç»“æœ:")
        for i, result in enumerate(results):
            status = "âœ…" if result.get('success', False) else "âŒ"
            url = urls[i] if i < len(urls) else "æœªçŸ¥"
            summary = result.get('summary', result.get('error', 'æ— ä¿¡æ¯'))
            print(f"  {status} {url}")
            print(f"     {summary}")
            
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        crawler.stop_all_tasks()
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")


def show_statistics(crawler: ImageCrawler):
    """æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = crawler.get_statistics()
        
        print("\n" + "="*60)
        print("ğŸ“Š çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯")
        print("="*60)
        
        print(f"ğŸ“¸ å›¾ç‰‡ç»Ÿè®¡:")
        print(f"  â€¢ æ€»å›¾ç‰‡æ•°: {stats['total_images']}")
        print(f"  â€¢ å·²ä¸‹è½½: {stats['downloaded_images']}")
        print(f"  â€¢ ä¸‹è½½ç‡: {stats['downloaded_images']/max(stats['total_images'], 1)*100:.1f}%")
        
        print(f"\nğŸ”„ ä¼šè¯ç»Ÿè®¡:")
        print(f"  â€¢ æ€»ä¼šè¯æ•°: {stats['total_sessions']}")
        print(f"  â€¢ æ´»è·ƒä»»åŠ¡: {stats['active_tasks']}")
        
        print(f"\nğŸ’¾ æ•°æ®åº“ä¿¡æ¯:")
        db_info = stats.get('database_info', {})
        if isinstance(db_info.get('tables'), dict):
            tables = db_info['tables']
            print(f"  â€¢ å›¾ç‰‡è®°å½•: {tables.get('images', 0)}")
            print(f"  â€¢ åˆ†ç±»è®°å½•: {tables.get('categories', 0)}")
            print(f"  â€¢ æ ‡ç­¾è®°å½•: {tables.get('tags', 0)}")
            print(f"  â€¢ ä¼šè¯è®°å½•: {tables.get('crawl_sessions', 0)}")
        
        print("="*60)
        
    except Exception as e:
        print(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")


async def clean_data(crawler: ImageCrawler):
    """æ¸…ç†æ•°æ®"""
    print("\nğŸ§¹ æ•°æ®æ¸…ç†")
    print("è¯·é€‰æ‹©è¦æ¸…ç†çš„å†…å®¹:")
    print("1. æ¸…ç©ºä¸‹è½½çš„å›¾ç‰‡æ–‡ä»¶")
    print("2. æ¸…ç©ºæ•°æ®åº“è®°å½•")
    print("3. æ¸…ç©ºæ‰€æœ‰æ•°æ®ï¼ˆå›¾ç‰‡+æ•°æ®åº“ï¼‰")
    print("4. è¿”å›ä¸»èœå•")

    choice = input("\nè¯·é€‰æ‹© (1-4): ").strip()

    if choice == '4':
        return

    clean_images = choice in ['1', '3']
    clean_database = choice in ['2', '3']

    if not clean_images and not clean_database:
        print("âŒ æ— æ•ˆé€‰é¡¹")
        return

    # ç¡®è®¤æ“ä½œ
    operations = []
    if clean_images:
        operations.append("ä¸‹è½½çš„å›¾ç‰‡æ–‡ä»¶")
    if clean_database:
        operations.append("æ•°æ®åº“è®°å½•")

    print(f"\nâš ï¸  å³å°†æ¸…ç©º: {', '.join(operations)}")
    confirm = input("ç¡®å®šè¦ç»§ç»­å—ï¼Ÿæ­¤æ“ä½œä¸å¯æ¢å¤ (y/N): ").strip().lower()

    if confirm != 'y':
        print("âŒ æ“ä½œå·²å–æ¶ˆ")
        return

    try:
        print("\nğŸ”„ æ­£åœ¨æ¸…ç†...")

        result = {
            'success': True,
            'images_deleted': 0,
            'database_cleared': False,
            'records_deleted': 0
        }

        # æ¸…ç©ºå›¾ç‰‡æ–‡ä»¶
        if clean_images:
            images_path = Path(crawler.settings.crawler.download_path)
            if images_path.exists():
                import shutil
                deleted_count = 0

                # è®¡ç®—è¦åˆ é™¤çš„æ–‡ä»¶æ•°
                for file_path in images_path.rglob('*'):
                    if file_path.is_file():
                        deleted_count += 1

                # åˆ é™¤æ•´ä¸ªç›®å½•
                shutil.rmtree(images_path)

                # é‡æ–°åˆ›å»ºç©ºç›®å½•
                images_path.mkdir(parents=True, exist_ok=True)

                result['images_deleted'] = deleted_count
                print(f"âœ… å·²åˆ é™¤ {deleted_count} ä¸ªå›¾ç‰‡æ–‡ä»¶")

        # æ¸…ç©ºæ•°æ®åº“è®°å½•
        if clean_database:
            from database.models.image import ImageModel
            from database.models.category import CategoryModel, TagModel
            from database.models.crawl_session import CrawlSessionModel

            # ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨çš„ä¼šè¯ä¸Šä¸‹æ–‡
            with crawler.db_manager.get_session() as session:
                records_deleted = 0

                # åˆ é™¤å›¾ç‰‡è®°å½•
                image_count = session.query(ImageModel).count()
                session.query(ImageModel).delete()
                records_deleted += image_count

                # åˆ é™¤çˆ¬å–ä¼šè¯è®°å½•
                session_count = session.query(CrawlSessionModel).count()
                session.query(CrawlSessionModel).delete()
                records_deleted += session_count

                # åˆ é™¤æ ‡ç­¾è®°å½•
                tag_count = session.query(TagModel).count()
                session.query(TagModel).delete()
                records_deleted += tag_count

                # é‡ç½®åˆ†ç±»è®°å½•ï¼ˆä¿ç•™é»˜è®¤åˆ†ç±»ï¼Œä½†æ¸…ç©ºç»Ÿè®¡ï¼‰
                categories = session.query(CategoryModel).all()
                for category in categories:
                    category.image_count = 0
                    category.total_size = 0

                session.commit()
                result['database_cleared'] = True
                result['records_deleted'] = records_deleted
                print(f"âœ… å·²æ¸…ç©º {records_deleted} æ¡æ•°æ®åº“è®°å½•")

        print("\nâœ… æ¸…ç†å®Œæˆ!")

    except Exception as e:
        print(f"\nâŒ æ¸…ç†å¤±è´¥: {e}")


def generate_config():
    """ç”Ÿæˆé…ç½®æ–‡ä»¶"""
    try:
        from shutil import copy2

        template_path = Path(__file__).parent / 'config' / 'templates' / 'config.yaml'
        output_path = Path('config.yaml')

        if output_path.exists():
            overwrite = input(f"é…ç½®æ–‡ä»¶ {output_path} å·²å­˜åœ¨ï¼Œæ˜¯å¦è¦†ç›–? (y/N): ").strip().lower()
            if overwrite != 'y':
                print("âŒ æ“ä½œå·²å–æ¶ˆ")
                return

        copy2(template_path, output_path)
        print(f"âœ… é…ç½®æ–‡ä»¶å·²ç”Ÿæˆ: {output_path}")
        print("è¯·æ ¹æ®éœ€è¦ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°")

    except Exception as e:
        print(f"âŒ ç”Ÿæˆé…ç½®æ–‡ä»¶å¤±è´¥: {e}")


async def test_system(crawler: ImageCrawler):
    """æµ‹è¯•ç³»ç»Ÿ"""
    print("ğŸ”§ æ­£åœ¨æµ‹è¯•ç³»ç»Ÿ...")
    
    try:
        # æµ‹è¯•æ•°æ®åº“è¿æ¥
        print("ğŸ“Š æµ‹è¯•æ•°æ®åº“è¿æ¥...", end='')
        if crawler.db_manager.test_connection():
            print(" âœ…")
        else:
            print(" âŒ")
            return
        
        # æµ‹è¯•ç½‘ç»œè¿æ¥
        print("ğŸŒ æµ‹è¯•ç½‘ç»œè¿æ¥...", end='')
        import aiohttp
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get('https://httpbin.org/get', timeout=10) as response:
                    if response.status == 200:
                        print(" âœ…")
                    else:
                        print(f" âŒ (HTTP {response.status})")
        except Exception as e:
            print(f" âŒ ({e})")
        
        print("âœ… ç³»ç»Ÿæµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


async def disaster_recovery_management(crawler):
    """å®¹ç¾å¤‡ä»½ç®¡ç†"""
    while True:
        print("\n" + "="*60)
        print("ğŸ›¡ï¸ å®¹ç¾å¤‡ä»½ç®¡ç†")
        print("="*60)

        # æ˜¾ç¤ºå½“å‰çŠ¶æ€
        dr_status = crawler.get_disaster_recovery_status()

        if not dr_status.get('enabled', False):
            print("âŒ å®¹ç¾å¤‡ä»½åŠŸèƒ½æœªå¯ç”¨")
            print("\nè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­å¯ç”¨å®¹ç¾å¤‡ä»½åŠŸèƒ½:")
            print("disaster_recovery:")
            print("  enabled: true")
            input("\næŒ‰å›è½¦é”®è¿”å›ä¸»èœå•...")
            return

        print("ğŸ“Š å½“å‰çŠ¶æ€:")

        # æ•°æ®åº“çŠ¶æ€
        db_status = dr_status.get('database_status', {})
        if 'disaster_recovery' in db_status:
            dr_info = db_status['disaster_recovery']
            print(f"  å½“å‰ä¸»æ•°æ®åº“: {dr_info.get('current_primary', 'N/A')}")
            print(f"  è‡ªåŠ¨å¤‡ä»½: {'âœ… å¯ç”¨' if dr_info.get('backup_enabled') else 'âŒ ç¦ç”¨'}")
            print(f"  å¥åº·ç›‘æ§: {'âœ… å¯ç”¨' if dr_info.get('monitoring_enabled') else 'âŒ ç¦ç”¨'}")
            print(f"  è‡ªåŠ¨æ•…éšœè½¬ç§»: {'âœ… å¯ç”¨' if dr_info.get('failover_enabled') else 'âŒ ç¦ç”¨'}")

        # å¥åº·çŠ¶æ€
        health_status = dr_status.get('health_status', {})
        if isinstance(health_status, dict) and health_status:
            print("\nğŸ” å¥åº·çŠ¶æ€:")
            for db_name, status in health_status.items():
                if isinstance(status, dict):
                    status_icon = "ğŸŸ¢" if status.get('status') == 'healthy' else "ğŸ”´"
                    print(f"  {status_icon} {db_name}: {status.get('status', 'unknown')}")
                    if 'response_time' in status:
                        print(f"    å“åº”æ—¶é—´: {status['response_time']:.2f}ms")

        # æ•…éšœè½¬ç§»çŠ¶æ€
        failover_status = dr_status.get('failover_status', {})
        if failover_status.get('enabled', False):
            print(f"\nâš¡ æ•…éšœè½¬ç§»çŠ¶æ€: {failover_status.get('current_status', 'unknown')}")

            failure_counts = failover_status.get('failure_counts', {})
            if failure_counts:
                print("  å¤±è´¥è®¡æ•°:")
                for db_name, count in failure_counts.items():
                    print(f"    {db_name}: {count}")

        # æœ€è¿‘çš„æ•…éšœè½¬ç§»å†å²
        history = dr_status.get('failover_history', [])
        if history:
            print(f"\nğŸ“‹ æœ€è¿‘æ•…éšœè½¬ç§»è®°å½• (æœ€è¿‘{len(history)}æ¡):")
            for i, event in enumerate(history[:3], 1):
                print(f"  {i}. {event['timestamp'][:19]}")
                print(f"     {event['source_db']} -> {event['target_db']}")
                print(f"     çŠ¶æ€: {event['status']}")

        # æ“ä½œèœå•
        print("\n" + "-"*40)
        print("æ“ä½œé€‰é¡¹:")
        print("1. åˆ›å»ºæ‰‹åŠ¨å¤‡ä»½")
        print("2. æŸ¥çœ‹è¯¦ç»†çŠ¶æ€")
        print("3. æ‰‹åŠ¨æ•…éšœè½¬ç§»")
        print("4. å¯ç”¨/ç¦ç”¨è‡ªåŠ¨æ•…éšœè½¬ç§»")
        print("5. æŸ¥çœ‹æ•…éšœè½¬ç§»å†å²")
        print("6. è¿”å›ä¸»èœå•")

        choice = input("\nè¯·é€‰æ‹©æ“ä½œ (1-6): ").strip()

        if choice == '1':
            await create_manual_backup(crawler)
        elif choice == '2':
            show_detailed_dr_status(crawler)
        elif choice == '3':
            await manual_failover(crawler)
        elif choice == '4':
            toggle_auto_failover(crawler)
        elif choice == '5':
            show_failover_history(crawler)
        elif choice == '6':
            break
        else:
            print("âŒ æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°é€‰æ‹©")

        input("\næŒ‰å›è½¦é”®ç»§ç»­...")


async def create_manual_backup(crawler):
    """åˆ›å»ºæ‰‹åŠ¨å¤‡ä»½"""
    print("\nğŸ”„ åˆ›å»ºæ‰‹åŠ¨å¤‡ä»½...")

    backup_name = input("è¯·è¾“å…¥å¤‡ä»½åç§° (ç•™ç©ºä½¿ç”¨é»˜è®¤åç§°): ").strip()
    if not backup_name:
        backup_name = None

    try:
        backup_path = crawler.db_manager.create_backup(backup_name)
        if backup_path:
            print(f"âœ… å¤‡ä»½åˆ›å»ºæˆåŠŸ: {backup_path}")
        else:
            print("âŒ å¤‡ä»½åˆ›å»ºå¤±è´¥")
    except Exception as e:
        print(f"âŒ å¤‡ä»½åˆ›å»ºå¤±è´¥: {e}")


def show_detailed_dr_status(crawler):
    """æ˜¾ç¤ºè¯¦ç»†çš„å®¹ç¾çŠ¶æ€"""
    print("\nğŸ“Š è¯¦ç»†å®¹ç¾çŠ¶æ€:")

    try:
        # ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·æ˜¾ç¤ºè¯¦ç»†çŠ¶æ€
        import subprocess
        result = subprocess.run(
            ["python", "disaster_recovery.py", "status"],
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='ignore',  # å¿½ç•¥ç¼–ç é”™è¯¯
            cwd=Path(__file__).parent
        )

        if result.returncode == 0:
            print(result.stdout)
        else:
            print(f"âŒ è·å–çŠ¶æ€å¤±è´¥: {result.stderr}")

    except Exception as e:
        print(f"âŒ è·å–è¯¦ç»†çŠ¶æ€å¤±è´¥: {e}")

        # å¦‚æœsubprocesså¤±è´¥ï¼Œç›´æ¥æ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯
        try:
            dr_status = crawler.get_disaster_recovery_status()
            if dr_status.get('enabled', False):
                print("\n=== å®¹ç¾å¤‡ä»½ç³»ç»ŸçŠ¶æ€ ===")

                # æ•°æ®åº“çŠ¶æ€
                db_status = dr_status.get('database_status', {})
                if 'disaster_recovery' in db_status:
                    dr_info = db_status['disaster_recovery']
                    print(f"å½“å‰ä¸»æ•°æ®åº“: {dr_info.get('current_primary', 'N/A')}")
                    print(f"è‡ªåŠ¨å¤‡ä»½: {'å¯ç”¨' if dr_info.get('backup_enabled') else 'ç¦ç”¨'}")
                    print(f"å¥åº·ç›‘æ§: {'å¯ç”¨' if dr_info.get('monitoring_enabled') else 'ç¦ç”¨'}")
                    print(f"è‡ªåŠ¨æ•…éšœè½¬ç§»: {'å¯ç”¨' if dr_info.get('failover_enabled') else 'ç¦ç”¨'}")

                # æ˜¾ç¤ºæ‰€æœ‰æ•°æ®åº“å®ä¾‹
                print("\n--- æ•°æ®åº“å®ä¾‹ ---")
                for db_name, info in db_status.items():
                    if db_name != 'disaster_recovery' and isinstance(info, dict):
                        status_icon = "ğŸŸ¢" if info.get('is_connected') else "ğŸ”´"
                        primary_icon = "ğŸ‘‘" if info.get('is_primary') else ""
                        print(f"  {status_icon} {db_name} {primary_icon}")
                        print(f"    ç±»å‹: {info.get('type', 'N/A')}")
                        print(f"    ä¼˜å…ˆçº§: {info.get('priority', 'N/A')}")
                        print(f"    è¿æ¥çŠ¶æ€: {'æ­£å¸¸' if info.get('is_connected') else 'å¼‚å¸¸'}")
                        if info.get('last_error'):
                            print(f"    æœ€åé”™è¯¯: {info['last_error']}")
            else:
                print("âŒ å®¹ç¾å¤‡ä»½åŠŸèƒ½æœªå¯ç”¨")
        except Exception as fallback_error:
            print(f"âŒ æ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯å¤±è´¥: {fallback_error}")


async def manual_failover(crawler):
    """æ‰‹åŠ¨æ•…éšœè½¬ç§»"""
    print("\nâš¡ æ‰‹åŠ¨æ•…éšœè½¬ç§»")

    # è·å–å¯ç”¨çš„æ•°æ®åº“åˆ—è¡¨
    dr_status = crawler.get_disaster_recovery_status()
    db_status = dr_status.get('database_status', {})

    available_dbs = []
    current_primary = None

    for db_name, info in db_status.items():
        if db_name != 'disaster_recovery' and isinstance(info, dict):
            if info.get('is_primary'):
                current_primary = db_name
            elif info.get('is_connected'):
                available_dbs.append(db_name)

    if not available_dbs:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„å¤‡ç”¨æ•°æ®åº“")
        return

    print(f"å½“å‰ä¸»æ•°æ®åº“: {current_primary}")
    print("å¯ç”¨çš„å¤‡ç”¨æ•°æ®åº“:")
    for i, db_name in enumerate(available_dbs, 1):
        print(f"  {i}. {db_name}")

    try:
        choice = int(input(f"\nè¯·é€‰æ‹©ç›®æ ‡æ•°æ®åº“ (1-{len(available_dbs)}): ").strip())
        if 1 <= choice <= len(available_dbs):
            target_db = available_dbs[choice - 1]
            reason = input("è¯·è¾“å…¥æ•…éšœè½¬ç§»åŸå›  (å¯é€‰): ").strip() or "æ‰‹åŠ¨æ•…éšœè½¬ç§»"

            # ç¡®è®¤æ“ä½œ
            confirm = input(f"âš ï¸ ç¡®è®¤è¦åˆ‡æ¢åˆ° '{target_db}'ï¼Ÿ(y/N): ").strip().lower()
            if confirm == 'y':
                print(f"ğŸ”„ æ­£åœ¨åˆ‡æ¢åˆ° {target_db}...")
                success = crawler.db_manager.manual_failover(target_db, reason)
                if success:
                    print(f"âœ… æ•…éšœè½¬ç§»æˆåŠŸ: {current_primary} -> {target_db}")
                else:
                    print(f"âŒ æ•…éšœè½¬ç§»å¤±è´¥")
            else:
                print("æ“ä½œå·²å–æ¶ˆ")
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
    except ValueError:
        print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
    except Exception as e:
        print(f"âŒ æ•…éšœè½¬ç§»å¤±è´¥: {e}")


def toggle_auto_failover(crawler):
    """åˆ‡æ¢è‡ªåŠ¨æ•…éšœè½¬ç§»çŠ¶æ€"""
    print("\nâš™ï¸ è‡ªåŠ¨æ•…éšœè½¬ç§»è®¾ç½®")

    dr_status = crawler.get_disaster_recovery_status()
    failover_status = dr_status.get('failover_status', {})

    if not failover_status.get('enabled', False):
        print("âŒ æ•…éšœè½¬ç§»åŠŸèƒ½æœªå¯ç”¨")
        return

    current_status = failover_status.get('auto_failover_enabled', False)
    print(f"å½“å‰çŠ¶æ€: {'âœ… å¯ç”¨' if current_status else 'âŒ ç¦ç”¨'}")

    action = "ç¦ç”¨" if current_status else "å¯ç”¨"
    confirm = input(f"æ˜¯å¦è¦{action}è‡ªåŠ¨æ•…éšœè½¬ç§»ï¼Ÿ(y/N): ").strip().lower()

    if confirm == 'y':
        try:
            if current_status:
                crawler.db_manager.disable_auto_failover()
                print("âœ… è‡ªåŠ¨æ•…éšœè½¬ç§»å·²ç¦ç”¨")
            else:
                crawler.db_manager.enable_auto_failover()
                print("âœ… è‡ªåŠ¨æ•…éšœè½¬ç§»å·²å¯ç”¨")
        except Exception as e:
            print(f"âŒ æ“ä½œå¤±è´¥: {e}")
    else:
        print("æ“ä½œå·²å–æ¶ˆ")


def show_failover_history(crawler):
    """æ˜¾ç¤ºæ•…éšœè½¬ç§»å†å²"""
    print("\nğŸ“‹ æ•…éšœè½¬ç§»å†å²")

    try:
        history = crawler.db_manager.get_failover_history(20)

        if not history:
            print("æš‚æ— æ•…éšœè½¬ç§»è®°å½•")
            return

        print(f"å…± {len(history)} æ¡è®°å½•:\n")

        for i, event in enumerate(history, 1):
            print(f"{i}. {event['timestamp']}")
            print(f"   {event['source_db']} -> {event['target_db']}")
            print(f"   åŸå› : {event['reason']}")
            print(f"   çŠ¶æ€: {event['status']}")
            if event.get('duration'):
                print(f"   è€—æ—¶: {event['duration']:.2f}ç§’")
            if event.get('error_message'):
                print(f"   é”™è¯¯: {event['error_message']}")
            print()

    except Exception as e:
        print(f"âŒ è·å–å†å²è®°å½•å¤±è´¥: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    print_banner()
    
    # åˆå§‹åŒ–çˆ¬è™«
    print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿ...")
    try:
        crawler = ImageCrawler()
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\n")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # ä¸»å¾ªç¯
    while True:
        try:
            choice = print_menu()
            
            if choice == '1':
                await crawl_single_website(crawler)
            elif choice == '2':
                await batch_crawl_websites(crawler)
            elif choice == '3':
                show_statistics(crawler)
            elif choice == '4':
                await disaster_recovery_management(crawler)
            elif choice == '5':
                await clean_data(crawler)
            elif choice == '6':
                generate_config()
            elif choice == '7':
                await test_system(crawler)
            elif choice == '8':
                print("ğŸ‘‹ å†è§!")
                # åœæ­¢å®¹ç¾å¤‡ä»½ç›‘æ§
                try:
                    crawler.stop_disaster_recovery()
                except:
                    pass
                break
            else:
                print("âŒ æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°é€‰æ‹©")
            
            input("\næŒ‰å›è½¦é”®ç»§ç»­...")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§!")
            break
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
            input("\næŒ‰å›è½¦é”®ç»§ç»­...")


if __name__ == '__main__':
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ å†è§!")
    except Exception as e:
        print(f"âŒ ç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
